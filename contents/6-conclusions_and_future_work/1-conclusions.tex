This thesis work posed two \glspl{RQ} defined in Section \ref{subsec:researchQuestion}. These were: 
\begin{enumerate}
    \item[RQ1:] How can we add support for \gls{HDFS} and \gls{HopsFS} to the delta-rs library to enable reading and writing on Delta Lake tables on the Hopsworks offline feature store?
    \item[RQ2:] What is the difference in read and write latency and throughput between the current legacy system operating on the Hopsworks offline feature store and the delta-rs library operating on HopsFS?
\end{enumerate}
The work conducted in this thesis answered these two questions by performing a system implementation and then evaluating the newly implemented system. 

The first step was adding support for \gls{HopsFS} in the delta-rs library. This was achieved by performing two code implementations of which the last one was used in the experiments. These two iterations proved necessary as \gls{HDFS} support was added to the delta-rs library during development, so to fit the consistency and maintainability non-functional requirements, modifying that approach to support \gls{HopsFS} was preferred. These code contributions are of more than two thousand \gls{LOC} for the first implementation and eight hundred for the second. While these metrics presented may offer some insight into the value of the contribution, on the other hand, the true worth of this work lies in the development of a production-ready solution that successfully navigates a complex data stack with intricate interdependencies, while meeting all specified requirements. The most significant acknowledgment of this contribution is the incorporation of this code implementation into the production environment of the Hopsworks feature store shortly after the publication of the thesis.

The second step was measuring and comparing the newly implemented system's performance with the legacy system. The metrics used were the latency (seconds) of the read and write operations and the throughput (rows/second), which was calculated by dividing the table size (in rows) by the latency of the operation. The results presented in Section \ref{sec:major_res} revealed that the delta-rs-based access to Delta Lake has a latency at least ten times lower in both read and write operations for tables from 10K to 6M rows in size. The write experiments show that the delta-rs library performs up to forty times better with smaller tables (10K and 100K) while still outperforming by ten times the legacy pipeline on the largest table (60M rows). This difference suggests that larger tables might have a threshold where a Spark-based system would perform better, but more experiments with larger tables are needed to verify the trend.
Similarly, in the reading experiments, delta-rs outperforms the legacy pipeline more with smaller tables (10K and 100K rows), even if only by a factor of fifteen instead of forty, as seen for the writing experiments. This is probably caused by using a Spark alternative in the legacy system's reading process (Arrow Flight and DuckDB), which already improves Spark performances on smaller tables. One last notable finding on the difference between the newly implemented system and the legacy pipeline is the difference in scalability as resources increase. Experiments were conducted with an increasing number of \gls{CPU} cores (from one to eight \gls{CPU} cores), and the results showed that delta-rs, being a local process, is much more suited for making use of those resources with an up to 31\% reduction in latency during the writing and an 87\% reduction during reading.

Overall, the experiments' results recommend adopting the newly implemented system in the defined use case (Section \ref{subsec:use_case}), either as a replacement or an alternative for users who wish to store data in a Delta Table within the offline feature store.