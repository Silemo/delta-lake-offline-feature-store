This thesis work posed two \glspl{RQ} defined in Section \ref{subsec:researchQuestion}. These were: 
\begin{enumerate}
    \item[RQ1:] How can we add support for \gls{HDFS} and \gls{HopsFS} to the delta-rs library?
    \item[RQ2:] What is the difference in latency and throughput between the current legacy system (Spark-based in writing) reading and writing to Apache Hudi compared to a delta-rs library-based reading and writing to Delta Lake, in \gls{HopsFS}?
\end{enumerate}
The work conducted in this thesis answered these two questions, by performing a system implementation and then evaluating the newly implemented system. 

The first step was adding support for \gls{HopsFS} in the delta-rs library. This was achieved by modifying the hdfs-native \cite{binfordKimahrimanHdfsnative2024} library, which reimplements in Rust a \gls{HDFS} client. 

The second step was measuring the newly implemented system's performance and comparing it with the legacy system. The metrics used were the latency (seconds) of the read and write operations and the throughput (rows/second) which was calculated by dividing the table size (in rows) by the latency of the operation. The results presented in Chapter \ref{ch:results_and_analysis} revealed that the delta-rs-based access to Delta Lake has a latency at least ten times lower in both read and write operations for tables from 10K to 6M rows in size. Considering only the write experiments these show that the delta-rs library performs up to forty times better with smaller tables (10K and 100K), while still outperforming by ten times the legacy pipeline on the largest table (60M rows). This difference suggests that with larger tables there might be a threshold where a Spark-based system would perform better, but more experiments with larger tables are needed to verify the trend. Similarly in the reading experiments delta-rs outperforms the legacy pipeline more with smaller tables (10K and 100K rows), even if only by a factor of fifteen instead of forty seen for the writing experiments. This is probably caused by the use of a Spark alternative in the reading process of the legacy system (Arrow Flight and DuckDB), which already improves Spark performances on smaller tables. One last notable finding on the difference between the newly implemented system and the legacy pipeline is the difference in scalability as resources increase. Experiments were conducted with an increasing number of \gls{CPU} cores (from 1 to 8 \gls{CPU} cores) and the results showed that delta-rs, being a local process, is much more suited for making use of those resources with an up to 31\% reduction in latency during writing and a 87\% reduction during reading.

Overall, the experiments' results recommend the adoption of the newly implemented system in the defined use case (Section \ref{subsec:use_case}), either in substitution or as an alternative to provide the user if they want to save the data on a Delta Table in the Offline Feature Store.
