Lakehouse systems are increasingly becoming the primary choice for running analytics in large-sized companies (that have more than 1000 employees) \cite{StateDataLakehouse2024}. 

This recent architecture design called Lakehouse \cite{lakehouse2021} is preferred over old paradigms, i.e. data warehouses and data lakes, as it builds upon the advantages of both systems, having the scalability properties of data lakes while preserving the \gls{ACID} properties typical of data warehouses \cite{lakehouse2021}. Additionally, Lakehouse systems include partitioning, which reduces query complexity significantly and provides "time travel" capabilities, enabling users to access different versions of data, versioned over time \cite{crociDataLakehouseHype2022}.

Three main implementations of this paradigm emerged over time \cite{ApacheHudiVs}: 
\begin{enumerate}
    \item \textbf{Apache Hudi}: first introduced by Uber, now primarily backed by Uber, Tencent, Alibaba, and Bytedance.
    \item \textbf{Apache Iceberg}: first introduced by Netflix and now primarily backed by Netflix, Apple, and Tencent.
    \item \textbf{Delta Lake}: first introduced by Databricks and now primarily backed by Databricks and Microsoft.
\end{enumerate}

While large communities back all three projects, Delta Lake is acknowledged as the de-facto Lakehouse solution \cite{ApacheHudiVs}. This is mainly thanks to Databricks, which first promoted this new architecture over data lakes among their clients around 2020 \cite{armbrustDeltaLakeHighperformance2020}.

As a data query and processing engine, Delta Lake is typically used with Apache Spark \cite{zahariaApacheSparkUnified2016}. This approach is effective when processing large quantities of data (1 TB or more) over the cloud, but whether this approach is effective on small quantities of data (100 GB or less) remains to be investigated \cite{Khazanchi1801362}.

DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019}, a \gls{DBMS} and Polars \cite{vinkWroteOneFastest2021}, a DataFrame library, highlighted the limitations of Apache Spark. When the data volume is small (between 1 GB and 100 GB) and the architecture is processing data locally, an Apache Spark cluster performs worse than alternatives. This ultimately brings an increase in costs and computation time ~\cite{BenchmarkResultsSpark,ebergenUpdatesH2OAi2023}.

Another aspect to keep in mind is that thanks to its ease of use and high abstraction level, Python has become the most used programming language in the data science space \cite{nagpalPythonDataAnalytics2019}. Python is currently the most popular programming language \cite{TIOBEIndex, StackOverflowDeveloper} and it is by far the most used language for \gls{ML} and \gls{AI} applications \cite{python-machine-learning}, this is mainly thanks to its strong abstraction capabilities and accessibility. This can also be observed by looking at the most popular libraries among developers, where two Python libraries make the podium: NumPy and Pandas \cite{StackOverflowDeveloper}.
In this scenario, creating a Python client for Delta Lake would be beneficial as it would not have to resort to Apache Spark and its Python \gls{API} (PySpark). This approach with small-scale use cases would improve performance significantly.

This native Python interface for Delta Lake directly benefits Hopsworks AB, the host company of this master thesis. Hopsworks AB develops a Feature Store for \gls{ML}, a centralized, collaborative data platform that enables to store and access of reusable features \cite{HopsworksBatchRealtime2024}. This architecture also supports point-in-time correct datasets from historical feature data \cite{Pettersson1695672}.

This project here presented, aims to speed up read and write operations on Delta Lake that acts as an Offline Feature Store in Hopsworks. Currently, the pipeline is Apache Spark-based and the key hypothesis of the project is that a faster non-Apache Spark alternative is possible. Ultimately if effective, this system implementation will become part of Hopsworks Feature Store (open source version), greatly improving the experience of Python users working on small quantities of data (between 10 GB and 100 GB) and also contributing to highlighting the limitations of the Apache Spark platform.