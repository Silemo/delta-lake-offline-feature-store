Data lakehouse systems are increasingly becoming the primary choice for running analytics in large companies with over 1000 employees \cite{StateDataLakehouse2024}. The data lakehouse architecture \cite{lakehouse2021} is preferred over old paradigms, i.e., data warehouses and data lakes, as it builds upon the advantages of both systems, having the scalability properties of data lakes while preserving the \gls{ACID} properties typical of data warehouses \cite{lakehouse2021}. Additionally, data lakehouse systems include partitioning, which reduces query complexity significantly and provides "time travel" capabilities, enabling users to access different versions of data, versioned over time \cite{crociDataLakehouseHype2022}.

Three main implementations of this paradigm emerged over time \cite{ApacheHudiVs}: 
\begin{enumerate}
    \item \textbf{Apache Hudi}: first introduced by Uber, now primarily backed by Uber, Tencent, Alibaba, and Bytedance \cite{rajaperumalUberEngineeringIncremental2017}.
    \item \textbf{Apache Iceberg}: first introduced by Netflix and now primarily backed by Netflix, Apple, and Tencent \cite{ApacheIcebergApache}.
    \item \textbf{Delta Lake}: first introduced by Databricks and now primarily backed by Databricks and Microsoft \cite{armbrustDeltaLakeHighperformance2020}.
\end{enumerate}

While large communities support all three projects, Delta Lake is acknowledged as the de-facto data lakehouse solution \cite{ApacheHudiVs}. This is mainly due to Databricks, which first promoted this new architecture over data lakes among their clients around 2020 \cite{armbrustDeltaLakeHighperformance2020}.

As a data query and processing engine, Delta Lake is typically used with Apache Spark \cite{zahariaApacheSparkUnified2016}. This approach is effective when processing large quantities of data (1 TB or more) in the cloud, but whether this approach is effective on small quantities of data (100 GB or less) remains to be investigated \cite{Khazanchi1801362}.

DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019}, a \gls{DBMS} and Polars \cite{vinkWroteOneFastest2021}, a DataFrame library, highlighted the limitations of Apache Spark. When the data volume is small (between 1 GB and 100 GB) and the architecture is processing data locally, an Apache Spark cluster performs worse than alternatives. This ultimately increases costs and computation time ~\cite{BenchmarkResultsSpark,ebergenUpdatesH2OAi2023}.

Another aspect to remember is that thanks to its ease of use and high abstraction level, Python has become the most used programming language in the data science space \cite{nagpalPythonDataAnalytics2019}. Python is currently the most popular general-purpose programming language \cite{TIOBEIndex, StackOverflowDeveloper}, and it is by far the most used language for \gls{ML} and \gls{AI} applications \cite{python-machine-learning}; this is mainly thanks to its strong abstraction capabilities and accessibility. This trend can also be observed by looking at the most popular libraries among developers, where two Python libraries make the podium: NumPy and Pandas \cite{StackOverflowDeveloper}.
In this scenario, using a Python client for Delta Lake would be beneficial as developers would not have to resort to Apache Spark and its Python \gls{API} (PySpark). This approach with small-scale (between 1 GB and 100 GB) use cases would improve performance significantly.

This native Python access for Delta Lake directly benefits Hopsworks \gls{AB}, the host company of this master thesis. Hopsworks \gls{AB} develops a homonymous Feature Store for \gls{ML}, a centralized, collaborative data platform that enables the storage and access of reusable features \cite{HopsworksBatchRealtime2024}. This architecture also supports point-in-time correct datasets from historical feature data \cite{Pettersson1695672}.

This presented project aims to reduce the latency (seconds) and thus increase the data throughput (rows/second) for reading and writing on Delta Lake tables that act as an offline feature store in Hopsworks. Currently, the writing pipeline is Apache Spark-based and the fundamental hypothesis of the project is that a faster non-Apache Spark alternative is possible. If successful, Hopsworks AB will consider incorporating this system implementation into the open-source Hopsworks Feature Store, significantly enhancing the experience for Python users working with smaller datasets (ranging from 1 GB to 100 GB). More generally, this work will outline the possibility of Apache Spark alternatives in small-scale use cases (between 1 GB and 100 GB).