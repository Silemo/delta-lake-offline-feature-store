Data lakehouse systems are increasingly becoming the primary choice for running analytics in large companies with over 1000 employees \cite{StateDataLakehouse2024}. The data lakehouse architecture \cite{lakehouse2021} is preferred over old paradigms, i.e., data warehouses and data lakes, as it builds upon the advantages of both systems, having the scalability properties of data lakes while preserving the \gls{ACID} properties typical of data warehouses \cite{lakehouse2021}. Additionally, data lakehouse systems include partitioning, which reduces query complexity significantly and provides "time travel" capabilities, enabling users to access different versions of data, versioned over time \cite{crociDataLakehouseHype2022}.

Three main implementations of this paradigm emerged over time \cite{ApacheHudiVs}: 
\begin{enumerate}
    \item \textbf{Apache Hudi}: first introduced by Uber \cite{rajaperumalUberEngineeringIncremental2017}, and now primarily backed by Uber, Tencent, Alibaba, and Bytedance.
    \item \textbf{Apache Iceberg}: first introduced by Netflix, and now primarily backed by Netflix, Apple, and Tencent.
    \item \textbf{Delta Lake}: first introduced by Databricks \cite{armbrustDeltaLakeHighperformance2020}, and now primarily backed by Databricks and Microsoft.
\end{enumerate}

While large communities support all three projects, Delta Lake is acknowledged as the de-facto data lakehouse solution \cite{ApacheHudiVs}. This recognition is mainly thanks to Databricks, which first promoted this new architecture over data lakes among their clients around 2020 \cite{armbrustDeltaLakeHighperformance2020}.

As a data query and processing engine, Delta Lake is typically used with Apache Spark \cite{zahariaApacheSparkUnified2016}. This approach is practical when processing large quantities of data (1 TB or more) in the cloud, but whether this approach is effective on a small scale (1 GB - 100 GB) remains to be investigated \cite{Khazanchi1801362}.

DuckDB~\cite{raasveldtDuckDBEmbeddableAnalytical2019}, a \gls{DBMS} and Polars~\cite{vinkWroteOneFastest2021}, a DataFrame library, highlighted the limitations of Apache Spark. When processing data locally with smaller volumes, an Apache Spark cluster underperforms compared to other alternatives. This result ultimately increases costs and computation time when using Spark~\cite{BenchmarkResultsSpark,ebergenUpdatesH2OAi2023}.

Another important consideration is that Python, due to its simplicity and high level of abstraction, has emerged as the most widely used programming language in the field of data science \cite{nagpalPythonDataAnalytics2019}. Python is currently the most popular general-purpose programming language \cite{TIOBEIndex, StackOverflowDeveloper}, and it is by far the most used language for \gls{ML} and \gls{AI} applications \cite{python-machine-learning}; this is mainly thanks to its strong abstraction capabilities and accessibility. This trend can also be observed by looking at the most popular libraries among developers, where two Python libraries make the podium: NumPy and Pandas \cite{StackOverflowDeveloper}.
In this scenario, using a Python client for Delta Lake would be beneficial as developers would not have to resort to Apache Spark and its Python \gls{API} (PySpark). This approach with small-scale (1 GB - 100 GB) use cases would improve performance significantly.

This native Python access for Delta Lake directly benefits Hopsworks \gls{AB}, the host company of this master thesis. Hopsworks \gls{AB} develops a homonymous feature store for \gls{ML}. This centralized, collaborative data platform enables the storage and access of reusable features~\footnote{Definition from the company's website at \url{https://www.hopsworks.ai/}}. This architecture also supports point-in-time correct datasets from historical feature data \cite{Pettersson1695672}.

This presented project aims to reduce the latency (seconds) and thus increase the data throughput (rows/second) for reading and writing on Delta Lake tables that act as an offline feature store in Hopsworks. Currently, the writing pipeline is Apache Spark-based, and the fundamental hypothesis of the project is that a faster non-Apache Spark alternative is possible. If successful, Hopsworks AB will consider incorporating this system into the open-source Hopsworks feature store, significantly enhancing the experience for Python users working with smaller datasets (1 GB - 100 GB). More generally, this work will outline the possibility of Apache Spark alternatives in small-scale use cases.