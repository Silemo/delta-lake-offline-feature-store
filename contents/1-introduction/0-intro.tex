Lakehouse systems are increasingly becoming the primary choice for running analytics in large sized companies (that have more than 1000 employees) \cite{StateDataLakehouse2024}. 

This recent architecture design \cite{lakehouse2021} is preferred over old paradigms, i.e. data warehouses and data lakes, because it takes the best of both worlds, having scalability properties of data lakes, while preserving the \gls{ACID} properties typical of data warehouses. Additionally, Lakehouse systems include partitioning, that reduces query significantly and time travel enabling users to access different versions of data, versioned over time \cite{crociDataLakehouseHype2022}.

Three implementations of this paradigm emerged over time \cite{ApacheHudiVs}: 
\begin{enumerate}
    \item \textbf{Apache Hudi}: first introduced by Uber, now primarily backed by Uber, Tencent, Alibaba and Bytedance
    \item \textbf{Apache Iceberg}: first introduced by Netflix and now primarily backed by Netflix, Apple and Tencent
    \item \textbf{Delta Lake}: first introduced by Databricks and now primarily backed by Databricks and Microsoft
\end{enumerate}

While all three projects are backed by large communities, it is Delta Lake that is more acknowledged as a Lakehouse solution \cite{ApacheHudiVs}. This is mainly thanks to Databricks, that first promoted this new architecture over data lakes among their clients around 2020 \cite{armbrustDeltaLakeHighperformance2020}.

Delta Lake is typically used in combination with Apache Spark \cite{zahariaApacheSparkUnified2016} that acts as a data query and processing engine. This approach is effective when processing large quantities of data (1 TB or more) over the cloud, but is this approach still effective in other scenarios such as local computing over small quantities of data (100 GB or less)?

In recent years, in other data storage cases, DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019} and Polars \cite{vinkWroteOneFastest2021}, showed as under a certain number of data volume, and in particular if the architecture is local, starting a Spark cluster, might actually reduce performance, increasing costs and the computation time. Spark alternatives at a smaller scale (10 GB - 100 GB) generally perform much better in these scenarios \cite{BenchmarkResultsSpark,ebergenUpdatesH2OAi2023}.

Another aspect to keep in mind when developing in this field is which programming language data scientists like to use, and this is Python. Python is currently the most popular programming language \cite{TIOBEIndex} and it is by far the most used language for \gls{ML} and \gls{AI} applications \cite{python-machine-learning}, this is mainly thanks to its strong abstraction capabilities and accessibility. This can be also observed by looking at the mentioned libraries, DuckDB, Polars and Spark, that all offer Python interfaces.
In this scenario, creating a Python client for Delta Lake would be beneficial as it would not have to resort to Spark and its Python library (PySpark). This approach with small-scale use cases would improve performance significantly.

This native Python interface for Delta Lake directly benefits Hopsworks AB, the host company of this master thesis. Hopsworks develops a \gls{ML} platform that enables developers to build, maintain and monitor \gls{ML} systems \cite{HopsworksBatchRealtime}. This implementation would be integrated into their \gls{ML} platform enabling developers using it, faster access to Delta Lake compared to Spark at a smaller data scale (typically between 10 GB and 100 GB).