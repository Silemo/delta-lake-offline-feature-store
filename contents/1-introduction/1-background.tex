Three key aspects of this project are essential to a comprehensive understanding: the development of the data lakehouse architecture, the significance and workflows of Spark, and the emergence of Python as a dominant programming language.

Data lakehouse is a term coined by Databricks in 2020 \cite{WhatLakehouse2020} to define a new design standard emerging in the industry. This new paradigm combined the capability of data lakes in storing and managing unstructured data with the \gls{ACID} properties typical of data warehouses.
Data warehouses became a dominant standard in the '90s, and early 2000s \cite{chaudhuriOverviewDataWarehousing1997}, enabling companies to generate \gls{BI} insights, managing different structured data sources. The problems related to this architecture were highlighted in the 2010s when the need to manage large quantities of unstructured data rose \cite{ederUnstructuredData802008}. 
So data lakes became the pool where all data could be stored, on top of which a more complex architecture could be built, consisting of data warehouses for \gls{BI} and \gls{ML} pipelines.
This architecture, while more suitable for unstructured data, introduces many complexities and costs related to the need to have replicated data (data lake and data warehouse) and several \gls{ELT} and \gls{ETL} computations.
Data lakehouse systems solved the problems of data lakes by implementing data management and performance features on top of open data formats such as Parquet \cite{DremelMadeSimple}. Three key technologies enabled this paradigm: (i) a metadata layer for data lakes, tracking which files are part of different tables; (ii) a new query engine design, providing optimizations such as \gls{RAM}/\gls{SSD} caching; and (iii) an accessible \gls{API} access for \gls{ML} and \gls{AI} applications. Uber first open-sourced this architecture design with Apache Hudi in 2017 \cite{rajaperumalUberEngineeringIncremental2017}, and then Databricks did the same with Delta Lake in 2020 \cite{armbrustDeltaLakeHighperformance2020}.

Spark is a distributed computing framework used to support large-scale data-intensive applications \cite{zaharia2010spark}. Developed as an evolution of the MapReduce paradigm, Spark has become the de-facto standard for big data processing due to its superior performance and versatility. Spark significantly improved its performance compared to its predecessor, i.e., Hadoop MapReduce (10 times better in its first iteration) \cite{zaharia2010spark} thanks to its use of in-memory processing. This feature means that Spark avoids going back and forth between storage disks to store the computation results. Spark, open-sourced under the Apache foundation as Apache Spark (from now on referred to as Spark), has seen widespread success and adoption in various applications, becoming the de-facto data-intensive computing platform for the distributed computing world. While Spark is often used as a comprehensive solution \cite{zahariaApacheSparkUnified2016}, different solutions might be better suited for a specific scenario.
An example of this is the case of Apache Flink \cite{carboneApacheFlinkStream}, designed for real-time data streams, which prevails over Spark where low latency real-time analytics are required. Similarly, Spark might not be the best tool for lower-scale applications where Spark's high-scaling capabilities may not be necessary. This is the case of DuckDB \cite{raasveldtDuckDBEmbeddableAnalytical2019} and Polars \cite{vinkWroteOneFastest2021}, that by focusing on a small-scale data (1 GB - 100 GB) provide a fast \gls{OLAP} embedded database and DataFrame management system respectively offering an overall faster computation compared to starting a Spark cluster for to perform the same operations. These technologies demonstrate that new applications outperforming Spark in specific use cases are possible and already in use. In particular, Apache Flink and DuckDB show that this is possible for real-time data streaming or small-scale computation. In this project, the latter use case is going to be explored.

Python can be considered the primary programming language among data scientists \cite{Python_CS-R9526}. Many first adopted Python thanks to its focus on ease of use, high abstraction level, and readability. These features helped create a fast-growing community behind the project, which led to the development of many libraries and \glspl{API}. So now, more than thirty years after its creation, it has become the de-facto standard for data science thanks to many daily used Python libraries such as TensorFlow, NumPy, SciPy, Pandas, PyTorch, Keras and many others. Python is also considered to be the most popular programming language, according to the number of results by search query (\textit{+"<language> programming"}) in 25 different search engines~\footnote{Evaluation methodology defined at \url{https://www.tiobe.com/tiobe-
index/programminglanguages_definition/}}. This ranking is computed yearly in the TIOBE Index \cite{TIOBEIndex}. The April 2024 rankings reveal that Python holds a rating of 16.41\%, followed by C at 10.21\%. The index also highlights trends from recent years, clearly illustrating Python's rise over traditionally popular languages like C and Java, both of which Python surpassed between 2021 and 2022. These scores underline the importance of providing Python \glspl{API}, particularly for programmers and data scientists, to enhance engagement and expand the capabilities of a framework.