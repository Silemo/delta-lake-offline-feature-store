%Summary here
The system evaluation complements the system implementation by measuring the performance of the developed solution, answering RQ2. This evaluation process will be carried out following a sequential approach.

This section details the research paradigm, the method, and the principles that will be used to carry out the system evaluation process, measuring the performance (latency, measured in seconds, and throughput, measured in rows/second) of reading and writing on Delta Lake or Apache Hudi while on \gls{HopsFS} of the current legacy pipeline (Spark-based in writing) and Rust-based pipelines. 

\subsection{Research Paradigm}
The research paradigm for the system evaluation section of this thesis has a more hybrid approach between positivism and post-positivism. While still performing a confirmatory research approach, based on defined objectives, it considers the limitations and biases of this approach, not seeking to generalize the results obtained to other cases. This approach is motivated by the limitations and biases given by the industrial context of this thesis while performing a confirmatory analysis based on a newly implemented system.

\subsection{Evaluation Process}
\label{subsec:eval_process}
The evaluation process will follow a sequential approach described in Figure \ref{fig:DevProcessRQ2}. Each step of this process is related to one of the goals (G5-G8) associated with the RQ2 in Section \ref{sec:goals}.
The relationship between each activity and the associated goal(s) is here explained:
\begin{enumerate}
    \item \textbf{Design experiments}: this activity maps perfectly to G5, designing the experiments that will be conducted to evaluate the performance difference in performance between the current legacy access (Spark-based in writing) to Apache Hudi compared a the delta-rs \cite{DeltaioDeltars2024} library-based access to Delta Lake, in \gls{HopsFS}. 
    \item \textbf{Perform experiments}: this activity maps perfectly to G6, using the code implementation (D1) to conduct the designed experiments on the analyzed systems. Here data is collected as latency expressed in seconds.
    \item \textbf{Transform data according to metrics}: this activity is requisite to fulfill G7, as throughput is computed from latency and not measured. The relationship that relates throughput (rows/second), latency(seconds), and size of table (rows) is the following:
    \[ throughput \; (rows/second) = \frac{number \; of \; rows \; (rows)}{latency \;(seconds)}\]
    \item \textbf{Visualize results}: this activity maps perfectly to G7, visualizing the experiments' result according to two metrics, i.e. latency measured in seconds and throughput measured in rows/second. This activity also generates a deliverable (D2) composed of the experiment results complete with tables and histograms, i.e. Chapter \ref{ch:results_and_analysis}.
    \item \textbf{Analyze results}: this activity maps perfectly to G8, analyzing and interpreting the results delivered in D2. This contributes to D3, generating the analysis of results, i.e. Chapter \ref{ch:results_and_analysis}.
\end{enumerate}
\begin{figure}[!ht]
    \begin{center}
      \includegraphics[width=\textwidth]{figures/3-method/research_process_rq2.png}
    \caption{\glstext{BPMN} diagram of the System evaluation process}
    \label{fig:DevProcessRQ2}
    \end{center}
\end{figure}

\subsection{Industrial use case}
\label{subsec:use_case}

For the system evaluation to be performed several choices must be made to select: which data is going to be used, which environment will run the experiments, and which metrics will be used to evaluate the system. While the other sections of this chapter detail which decisions were taken, this section aims to outline a typical use case of the system implementation that can motivate the choices between how the system is going to be evaluated.

During the research work in Hopsworks \gls{AB}, the author had the chance to talk to various employees and form an idea of what a typical client use case for the Hopsworks Feature Store looks like. While these parameters are qualitative, they depict a specific use case around which this thesis work was built. Here below are these use case characteristics:
\begin{itemize}
  \item \textbf{Usage of rows over storage size}: Contrary to the introduction chapter where the size of workload is mostly referred to by storage size (bytes), in the experimental part of the thesis only the number of rows will be used to refer to size. This is motivated by the need to find a reliable unit that measures a table size. Storage (bytes) is not strictly linked to a table structure (a table could have a lot of columns and a small number of rows and occupy the same memory as a table with a lot of rows but few columns), and it varies across platforms (different \gls{DBMS} might save information with different overheads) and storage structures (row-oriented format vs. column-oriented format, i.e. csv vs. parquet). Thus storage size (bytes) alone is unreliable and cannot be used to measure data pipeline performance. In this thesis, number of rows was kept as the main unit to measure data size, but it was also supplemented by specifying the number of columns and the data types contained in each row in Section \ref{subsec:dataset}.
  \item \textbf{Selected table size}: within Hopsworks the author had the chance to see that most of the company's clients' workloads were limited (from 1M to 100M rows), while few clients had massive workloads (more than 1 BN rows). Given this outlook, this project opted to work on improving performance for the smaller workloads setting the table sizes between 100K and 60M rows. This is further detailed in Section \ref{subsec:dataset}.
  \item \textbf{Type of data}: the Hopsworks Feature Store works only with structured data (e.g. numbers, strings), and not with unstructured data (e.g. images, videos, audio) so also the selected dataset in Section \ref{subsec:dataset} will reflect this scenario.
  \item \textbf{Client configuration}: the performance of the implemented system also depends on the computational and storage resources available on the client configuration running the system. To reflect a typical use case scenario, four \gls{CPU} configurations were selected (1 core, 2 cores, 4 cores, and 8 cores), while \gls{RAM} memory will be adapted to the need of the system (as it is unknown before running the experiments). Additionally, to avoid storage I/O bottlenecks and reflect a modern system, a system equipped with a \gls{SSD} storage will be used. This is further detailed in Section \ref{subsec:exp_env}.
\end{itemize}

\subsection{Dataset}
\label{subsec:dataset}

The data that will be used to perform the read and write operations comes from \glsentryshort{TPC}-H benchmark suite\cite{TPCHHomepage}. \glsentryshort{TPC}-H is a decision support benchmark by \gls{TPC}. It consists of a series of business-oriented ad-hoc queries designed to be industry-relevant \cite{transactionprocessingperformancecounciltpcTPCH_v301pdf1993}. The data coming from this benchmark suite was used as
it provides a recognized standard for data storage systems \cite{TPC_benchmarks_2000}, and it has already been used in similar related work \cite{raasveldtDuckDBEmbeddableAnalytical2019, behmPhotonFastQuery2022}. Any part of the data can be generated using the TPC-H data generation tool \cite{TPCCurrentSpecs}.

The \glsentryshort{TPC}-H benchmark contains eight tables, of these two, SUPPLIER and LINEITEM, were selected for the following reasons. The two tables are respectively the smallest (10000 rows) and largest (6000000 rows) tables whose size (number of rows) depends on the \gls{SF}. The \gls{SF} can be varied to obtain tables of different sizes (number of rows), allowing a progressive change in the table size (number of rows). 

The SUPPLIER table has seven columns, while the LINEITEM table has sixteen. This influences the average size of memory each row occupies. Here below for each table its columns, with their specific type, are listed.
\begin{itemize}
  \item SUPPLIER
  \begin{itemize}
    \item S\_SUPPKEY : identifier
    \item S\_NAME : fixed text, size 25
    \item S\_ADDRESS : variable text, size 40
    \item S\_NATIONKEY : identifier
    \item S\_PHONE : fixed text, size 15
    \item S\_ACCTBAL : decimal
    \item S\_COMMENT : variable text, size 101
  \end{itemize}
  \item LINEITEM
  \begin{itemize}
    \item L\_ORDERKEY : identifier
    \item L\_PARTKEY : identifier
    \item L\_SUPPKEY : identifier
    \item L\_LINENUMBER : integer
    \item L\_QUANTITY : decimal
    \item L\_EXTENDEDPRICE : decimal
    \item L\_DISCOUNT : decimal
    \item L\_TAX : decimal
    \item L\_RETURNFLAG : fixed text, size 1
    \item L\_LINESTATUS : fixed text, size 1
    \item L\_SHIPDATE : date
    \item L\_COMMITDATE : date
    \item L\_RECEIPTDATE : date
    \item L\_SHIPINSTRUCT : fixed text, size 25
    \item L\_SHIPMODE : fixed text, size 10
    \item L\_COMMENT : variable text size 44
  \end{itemize} 
\end{itemize}
As mentioned in Section \ref{subsec:use_case}, measuring the memory size in terms of bytes that a row of a table occupies has no single approach, as it depends on how the row is stored (e.g. a \gls{DBMS}, a row or column-oriented format). Thus no specific figure is given here, but all information on the data from how to retrieve it and how it is composed is provided so that a memory size in bytes can be calculated at need.

Considering the different number of columns of the two tables used, this means that the selected metrics, i.e. latency (seconds) and throughput(rows/second) cannot be used to compare results across different tables. This is the reason why the comparative evaluation only considers different configurations on the same table.

For this project, five table variations were used to benchmark the code solution as D1. \gls{SF} was varied to obtain a table at each significant figure, from 10000 rows to 60000000 rows. These are the tables:
\begin{enumerate}
    \item \textit{supplier\_sf1}: size = 10000 rows
    \item \textit{supplier\_sf10}: size = 100000 rows
    \item \textit{supplier\_sf100}: size = 1000000 rows
    \item \textit{lineitem\_sf1}: size = 60000000 rows
    \item \textit{lineitem\_sf10}: size = 60000000 rows
\end{enumerate}

\subsection{Experimental Design}
\label{subsec:experimental_design}
% Description of which benchmarks will be performed and how. Here it needs to be explained the environment where the benchmarks will be run (Snurran), which tests are run, and why in this particular way.
%
% Talk about the selection of the questions: How do the performances vary if: (1) the newly implemented pipeline is used vs. the old pipeline. (2) a simple local filesystem implementation is used vs. writing on \gls{HopsFS} (3) varying the CPU configuration (4) we have different table sizes. 

The experiments aim to highlight the differences between the newly implemented system based on the delta-rs library, and the current legacy system. To isolate the benefit of using delta-rs over Spark and provide a baseline, three different testing pipelines were designed:
\begin{enumerate}
    \item \textbf{delta-rs - \glsentryshort{HopsFS}}: the system implemented in Chapter \ref{ch:implementation}. It comprises a Rust pipeline with Python bindings, enabling performing operations (i.e., reading, writing) on Delta Lake tables. This pipeline writes on \gls{HopsFS}.
    \item \textbf{delta-rs - \glsentryshort{LocalFS}}: this pipeline uses the same library as the system implemented, but saves data on the \gls{LocalFS}. This provides a comparison within the delta-rs library, isolating the impact on performance caused by writing on \gls{HopsFS}, a distributed file system.
    \item \textbf{Legacy pipeline}: this pipeline uses the Hopsworks Feature Store to write data on Hudi tables. This system makes use of a pipeline based on Kafka, and Spark to write data on the Hudi tables, saved on \gls{HopsFS}. The pipeline uses a Spark alternative, namely DuckDB and Arrow Flight, to read data as explained in Section \ref{subsec:legacy_sys_reading}. 
\end{enumerate}

Furthermore, the experiments will verify how the performances of the three systems will change based on the \gls{CPU} resources provided (namely 1 core, 2 cores, 4 cores, 8 cores). Each time the testing environment will be modified accordingly, creating a new \gls{VM} where the experiments will run with increasingly more resources. These \gls{CPU} configurations were chosen together with the industrial supervisor, according to the typical Hopsworks use case (Section \ref{subsec:use_case}).

The data used for experiments, as described in Section \ref{subsec:dataset}, will come from two different tables. These are modified according to a \gls{SF}, for a total of five times.

Additionally, during the writing experiments performed using the legacy pipeline (Spark-based) the contribution of different parts of the process will be measured: namely the upload time and materialization time, dichotomy explained in Section \ref{subsec:legacy_sys_writing}. This will serve to verify how different parts of the legacy pipeline scaled with table sizes, and if Spark was the bottleneck of the architecture.

The Apache Hudi pipelines are preferred over the new Spark based pipeline reading and writing on Delta Lake because these were released and tested extensively on Hopsworks platform, so they provide more guarantees of obtaining relevant results. Additionally, at the time of experiment design these two variations, Spark writing on Delta Lake and Spark writing on Apache Hudi, were considered similarly in read and write performance. 

In conclusion, the experiments conducted will be a total of 3 (pipelines) times 4 (\gls{CPU} configurations) times 5 (tables) times 2 (read and write operations), i.e. 120 experiments, performed 50 times each to create statistically significant results.

\subsection{Experimental environment}
\label{subsec:exp_env}
% Describe Snurran

The experimental environment consists of a physical machine in Hopsworks' offices, virtualized enabling remote shared development. The \gls{CPU} details of the machine are present in Listing \ref{lst:cpu_snurran}, noting that only eight cores at maximum were accessed during the experiments. It should be observed that this experimental environment while virtualized in isolation, runs on shared computing resources, so experiment results might vary depending on the load of the machine. Considering this all experiments will be run when the machine load is low (less than 50\% of \gls{CPU} and \gls{RAM} usage), to avoid having results depend on external workloads running.

The machine mounts about 5.4 TBs of \gls{SSD} memory. This allows the machine to have fast read and write speed, 2.7 GB/s, and 1.9 GB/s respectively (measured with a simple \textit{dd} bash command). 

The experimental environment will be set up with a Jupyter Server of different CPU cores, depending on the experiment (1 core, 2 cores, 4 cores, or 8 cores). The Jupyter server is allocated by default with 2048 MB of \gls{RAM}, out of the 192 GB  available on the experimental machine. This amount will be adjusted during the experiments according to the needs of the experiments (see Section \ref{subsec:resources_usage}).

\begin{lstlisting}[language=bash, caption={Output of a \textit{lscpu} bash command on the test environment.}, label={lst:cpu_snurran}, frame=tb]
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         48 bits physical, 
                         48 bits virtual
  Byte Order:            Little Endian
CPU(s):                  32
  On-line CPU(s) list:   0-31
Vendor ID:               AuthenticAMD
  Model name:            AMD Ryzen Threadripper 
                         PRO 5955WX 16-Cores
    CPU family:          25
    Model:               8
    Thread(s) per core:  2
    Core(s) per socket:  16
    Socket(s):           1
    Stepping:            2
    Frequency boost:     enabled
    CPU max MHz:         7031.2500
    CPU min MHz:         1800.0000
    BogoMIPS:            7985.56
Virtualization features: 
  Virtualization:        AMD-V
Caches (sum of all):     
  L1d:                   512 KiB (16 instances)
  L1i:                   512 KiB (16 instances)
  L2:                    8 MiB (16 instances)
  L3:                    64 MiB (2 instances)
\end{lstlisting}

\subsection{Evaluation Framework}
% Describe which metrics are used to evaluate the system. Which metrics were used during the experiments
%
% What I would like to say is essentially that:
% This evaluation section's objective is to: Verify The system requirements defined in ... , and compare the performance (defined as throughput in RQ2) with older systems. Explain how both of these things are done.

The system evaluation framework is designed to evaluate three key aspects of the system, using different metrics:
\begin{enumerate}
    \item \textbf{Functional requirements}: defined in Section \ref{subsec:requirements}, functional requirements will be measured by verifying the success or failure of running an experiment. By design, this will not happen, as the system implementation phase, continues until all functional requirements are met.
    \item \textbf{Non-functional requirements}: defined in Section \ref{subsec:requirements}, non-functional requirements are: consistency, maintainability and scalability. The first two requirements are mainly addressed during implementation, while scalability is measured during the system evaluation experiments. The metric used for measuring this requirement is the throughput measured in rows/second as defined in RQ2.
    \item \textbf{How does the system compare to other pipelines?}: this question answers directly RQ2, measuring the throughput (rows/second) of the different pipelines, defined in Section \ref{subsec:experimental_design}. Results are then compared using a visual approach.
\end{enumerate} 

\subsection{Assessing Reliability and Validity}
\label{subsec:reliability_validity}
% Here explain how the readability and validity of the collected data were addressed. Mainly here the bootstrapping method needs to explained and how it achieves getting a CI without making assumption on the data distribution.

Results are significant according to their reliability and validity. In this project work, to ensure the reliability of the experiment results on the system performance, each experiment will be performed fifty times. This number was agreed as a balance between consistency and the limited resources available (in terms of time and computing resources).

Probably due to the complex nature of the pipeline tested, the data distribution of results could vary from one experiment to the other. This hampers the possibility of comparing results, greatly impacting the relevance of the results analysis. To restore the validity of the data collected a bootstrapping technique will be used. Data will be resampled with substitution a thousand times, then an average with a confidence interval for each experiment will be calculated. This will benefit the accuracy of the results presented.