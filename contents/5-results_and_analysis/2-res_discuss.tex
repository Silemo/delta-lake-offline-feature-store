This section discusses and analyses the results presented in Section \ref{sec:major_res}. Contrarily to Section \ref{sec:major_res} where results are grouped by experiment type (reading or writing), here each subsection is formed around an observation based on the results that is then motivated and explored, considering all relevant experiments.

\subsection{delta-rs performance on \glsentryshort{HopsFS} are similar to the library's performance on the \glsentryshort{LocalFS}}

The delta-rs on \gls{HopsFS} pipeline performs similarly (same significant figure) to the delta-rs on \gls{LocalFS} in all experiments performed on tables from 1M to 60M rows. This suggests that the developed solution takes advantage of the library's full potential. Smaller tables, i.e. 10K and 100K rows, still perform better on the \gls{LocalFS} (around 10 times better), but this should be caused by limitations given by the different nature of \gls{HopsFS}, i.e. a distributed system.

\subsection{delta-rs outperforms the Legacy Spark pipeline in every experiment}

The delta-rs on \gls{HopsFS} pipeline outperforms the Legacy Spark pipeline by a factor of ten or more in write operations and by a lower factor in read operations. While the trends suggest that with higher table dimensions there might be a cutoff between using the delta-rs pipeline and the legacy Spark pipeline, the implemented system greatly outperforms the legacy Spark system in the designed experiments. 

This suggests that in the use case defined in Section \ref{subsec:use_case}, which leads to the design of the experiments, delta-rs should be adopted as the preferred solution over the legacy Spark pipeline.

\subsection{delta-rs scales well with increased \glsentryshort{CPU} cores}

Both delta-rs pipelines scale considerably as the \gls{CPU} cores increase, by about 30\% (latency and throughput) in write operations and linearly for read operations (inverse for latency). On the other hand, the legacy Spark pipeline scales poorly, with limited improvements with more \gls{CPU} cores. 

This difference between the two systems is caused by the different architecture of the pipelines. Delta-rs performs operations (read or write) on the local machine. In contrast, the legacy Spark pipeline uses the local machine only to upload the table in Kafka as messages (this is why the upload time improves with more \gls{CPU} cores), while it uses other computations present on a Spark cluster to perform the read or write. This means that to improve performances there is the need to give more resources to the Spark cluster, which is an aspect not controlled in these experiments (reproducing a typical scenario in the case of a Hopsworks client's deployment). 

\subsection{The upload time in the legacy Spark pipeline becomes the bottleneck as table size increases}

The two steps of the legacy Spark application writing a table show the materialization time, i.e. when Spark computes, have a great contribution to the overall latency with smaller tables, but do not grow linearly with the table sizes. On the other hand, the upload time scales linearly with the table size from the 1M table, effectively composing 95\% of the overall time for the largest table (60M rows). This is caused by how the upload process was implemented in the legacy Spark pipeline, which iterates on each table row to upload them as Kafka messages. 

This highlights that the Legacy Spark pipeline while it might perform better than the implemented system (delta-rs on \gls{HopsFS}) with larger tables (more than 60 M rows), is limited in the write operation by the upload operation which scales linearly with a table size. Using a different approach to how data is uploaded in Kafka might improve performances on the legacy Spark pipeline.