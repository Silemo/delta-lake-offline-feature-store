This section discusses the main results presented in Section \ref{sec:major_res}, explaining their meaning, their implication for the company, and more generally for the research area. Additionally, this section provides a collection of project considerations outlined either during development or after the experiments conducted.

\subsection{Discussion on major results}

The results presented in Section \ref{sec:major_res} reveal the differences in latency (and thus also data throughput) between the newly implemented system using the delta-rs library and the legacy system. While the experiments present task-specific differences in performance between the two systems, overall, the system developed in this thesis has from ten up to forty times lower latency than the legacy system in all experiments when using the tables from 10K to 6M rows. These findings not only confirm the hypothesis that a Rust-based system would be faster than a Spark-based system when operating on small tables (from 10K to 6M rows) but also show that delta-rs is a preferable alternative to how read operations are currently performed using a combination of Arrow Flight and DuckDB (Section \ref{subsec:legacy_sys_reading}). Said outcomes further solidify the idea of the pivotal role that Rust will play in optimizing computer systems \cite{Balasub2017}.

Something more can be said by taking a more in-depth look at the results of the writing experiments. In writing experiments, even with the largest table (60M rows), the newly implemented system has a latency from ten up to forty times lower than the legacy system. When considering the smallest tables (10K and 100K rows), the improvement of the new system over the legacy pipeline is up to forty times. This confirms the need for Spark alternatives when elaborating on small-size data. For the use case defined in Section \ref{subsec:use_case}, it is clear that the new system is a better alternative in terms of performance but also costs, as maintaining a Spark cluster for this small amount of data makes little sense. 

Moving on to reading experiments, as explained in Section \ref{subsec:legacy_sys_reading}, the legacy system, when reading, is already using a Spark alternative, i.e., Arrow Flight and DuckDB. The results show that there is a smaller difference between the two systems when reading the largest table (60M rows) with only 46\% improvement. Nonetheless, the new system scales much better as the \gls{CPU} cores increase, up to 89.75\% with eight cores, while performance in the legacy system remains more or less the same. This has to do with how resources are allocated in the system. The user can dynamically modify his local resources, and since delta-rs uses local resources to operate, the user can tune the system as needed. On the other hand, the Arrow Flight server using DuckDB has a predefined amount of resources that cannot be modified as easily (the Hopsworks cluster needs to be restarted). Overall, the flexibility that the new system offers, at the expense of delegating computation on the client side, is remarkable and might benefit hybrid workflows.

These findings impact Hopsworks \gls{AB} considerably, as it affects their main product, i.e., the Hopsworks feature store. The results recommend adopting the developed system over the current system using Apache Hudi as an offline feature store. Considering the Hopsworks \gls{AB} approach of supporting multiple ways to load and save data, an alternative to the total substitution of the system would be leaving the option to the user to choose which data lakehouse format the offline feature store should save the data. It should be noted that the experiments and system evaluation were performed on the use case defined in Section \ref{subsec:use_case}. For different use cases, more experiments should be performed. The author expects that there will be a data size threshold where using a Spark-based will make more sense in terms of performance (lower latency).

More generally speaking, these findings cannot be generalized due to the intrinsic bias of conducting an industrial master thesis within the company developing the product to evaluate. Furthermore, the defined use case (Section \ref{subsec:use_case}) restricts the results on specific table sizes and computational resources. Results could vary if reading or writing more data with a different amount of resources. Nonetheless, the results confirm the research expectations, supporting the idea that Spark alternatives should be considered (and sometimes preferred) when working with small-sized tables (10K to 60M rows). This outcome is particularly relevant for Spark's numerous open-source community (more than 2800 contributors during its lifetime \cite{ApacheSparkOpen}). These findings also encourage more research and experiments on Spark alternatives and Rust applications in data management system implementations.

\subsection{Considerations on the legacy system}

The legacy system presented in Sections \ref{subsec:legacy_sys_writing} and \ref{subsec:legacy_sys_reading} has a layered architecture design to fit multiple needs. For example, Kafka is used as a single point of upload of data both to the offline feature store analyzed in this thesis work and the online feature store designed for streaming pipelines. Using Kafka ensures that data is consistent between the two feature stores (online and offline) but also represents a system performance limitation. As results in Figure \ref{fig:hudi_virtualiz_breakdown} and Table \ref{tbl:hudi_virtualiz_breakdown_cpu_perc} show, Kafka, as data increases, represents the bottleneck of the architecture, accounting for 95\% of the write latency when writing a 60M rows table. This mainly concerns how Kafka is used in the architecture and how data is sent, i.e., row by row. 
This work helped highlight this issue, but more research and experiments are required to find an effective solution. Enabling multiple uploads via concurrency mechanisms might speed up the upload process. Alternatively, sending columns instead of rows and keeping a column structure along the pipeline might also help. 

Another aspect that limited the capability of the legacy architecture is how resources are allocated for the Spark cluster. These can be modified more easily than the Arrow Flight server's computation resources. However, balancing two systems (the client's and the Spark client's) creates an added complexity that is unnecessary when elaborating on small quantities of data.

\subsection{Considerations on the delta-rs library}

The system implementation focus of this project was adding support for \gls{HopsFS} to the delta-rs library. This was also made possible thanks to the library's built-in modularity, which has a different sub-library for each storage connector. The community around the library is very active, and each question made on their communication channels, namely GitHub and Slack, would always receive an answer within a few hours or a day. The only recommendation the author would give to the library's maintainers would be to document further, perhaps with the help of architectural diagrams, the inner workings of the library's processes, specifying how and when data gets uploaded into memory. The first iteration of the read experiments had to be scratched due to calling a lazy function that would not load data into memory. 

Considering the results presented in Section \ref{sec:major_res}, the similarities in performance (latency and throughput) of the two delta-rs pipelines, operating on the \gls{LocalFS} and \gls{HopsFS} respectively, suggest that the library is operating at its full potential also in the newly implemented system. Delta-rs on the \gls{LocalFS} still performs faster, but this might have to do with the different nature of the two storage systems, i.e., local and distributed.