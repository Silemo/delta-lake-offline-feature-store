This section discusses the main results presented in Section \ref{sec:major_res}, trying to explain their meaning, their implication for the company, and more generally for the research area. Additionally, this section provides a collection of project considerations outlined either during development or after the experiments conducted.

\subsection{Discussion on main results}

The results presented in Section \ref{sec:major_res} reveal the differences in latency (and thus also data throughput) between the newly implemented system using the delta-rs library and the legacy system. While the experiments present task-specific differences in performance between the two systems, overall the system developed in this thesis work has at least ten times lower latency than the legacy system in all experiments when using the tables from 10K to 6M rows. These findings not only confirm the hypothesis that a Rust-based system would be faster than a Spark-based system when operating on small tables (from 10K to 6M rows) but also show that delta-rs is a preferable alternative to how read operations are currently performed using a combination of Arrow Flight and DuckDB (Section \ref{subsec:legacy_sys_reading}). Said outcomes further solidify the idea of the pivotal role that Rust will play in optimizing computer systems \cite{Balasub2017}.

Taking a more in-depth look at the results of the writing experiments, something more can be said. In writing experiments, even with the largest table (60M rows), the newly implemented system has a latency ten times lower than the legacy system. When considering the smallest tables (10K and 100K rows), the improvement of the new system over the legacy pipeline is up to forty times. This confirms even more the need for Spark alternatives when elaborating small-size data. For the use case defined in Section \ref{subsec:use_case} it is clear that the new system is a better alternative in terms of performance but also costs, as maintaining a Spark cluster for this small amount of data makes little sense. 

Moving on to reading experiments, as explained in Section \ref{subsec:legacy_sys_reading}, the legacy system when reading is already using a Spark alternative, i.e. Arrow Flight and DuckDB. The results show that there is a smaller difference between the two systems when reading the largest table (60M rows) with only 46\% improvement. Nonetheless, the new system scales much better as the \gls{CPU} cores increase, up to 89.75\% with eight cores, while performance in the legacy system remains more or less the same. This has to do with how resources are allocated in the system. The user can modify dynamically his local resources, and so since delta-rs uses local resources to operate the user can tune the system at his necessity. On the other hand, the Arrow Flight server using DuckDB has a predefined amount of resources available, that cannot be modified as easily (the Hopsworks cluster needs to be redeployed). Overall the flexibility that the new system offers, at the expense of delegating computation on the client side is remarkable and might benefit hybrid workflows.

The impact of these findings for Hopsworks \gls{AB} is considerable, as it affects their main product, i.e. the Hopsworks Feature Store. The results recommend adopting the developed system over the current system using Apache Hudi as an Offline Feature Store. Considering the Hopsworks \gls{AB} approach of supporting multiple ways to load and save data, an alternative to the total substitution of the system would be leaving the option to the user to choose which Lakehouse the Offline Feature Store should save the data. It should be noted that the experiments and system evaluation were performed on the use case defined in Section \ref{subsec:use_case}. For different use cases, more experiments should be performed. The author expects that there will be a data size threshold where using a Spark-based will make more sense in terms of performance (lower latency).

More generally speaking, these findings have little possibility of being generalized due to the intrinsic bias of conducting an industrial master thesis within the company developing the product to evaluate. Furthermore, the defined use case (Section \ref{subsec:use_case}) restricts the results on specific table sizes and computational resources. Results could vary if reading or writing more data with a different amount of resources. Nonetheless, the results confirm the research expectations, contributing to supporting the idea that Spark alternatives should be considered (and sometimes preferred) when working with small-sized tables (100K to 60M rows). This encourages more research and experiments to be conducted on Spark alternatives and Rust applications in data management system implementations. 

\subsection{Considerations on the legacy system}

The legacy system presented in Sections \ref{subsec:legacy_sys_writing} and \ref{subsec:legacy_sys_reading} has a layered architecture design to fit multiple needs. For example, Kafka is used as a single point of upload of data both to the Offline Feature Store analyzed in this thesis work and the Online Feature Store designed for streaming pipelines. Using Kafka ensures that data is consistent between the two Feature Stores (Online and Offline) but it also represents a limitation in the system performance. As results in Figure \ref{fig_tbl:hudi_virtualiz_breakdown} show, Kafka as data increases represents the bottleneck of the architecture, making 95\% of the write latency when writing a 60M rows table. This has mainly to do with how Kafka is used in the architecture and how data is sent, i.e. row by row. 
This work helped highlight this issue, but more research and experiments are required to find an effective solution. Enabling multiple uploads via concurrency mechanisms might speed up the upload process. Alternatively, sending columns instead of rows, and keeping a column structure along the pipeline might also help. 

Another aspect that limited the capability of the legacy architecture is how resources are allocated for the Spark cluster. These can be modified more accessibly compared to the computation resources for the Arrow Flight server, but having to balance two systems (the client's and the Spark client) creates an added complexity not necessary when elaborating small quantities of data.

\subsection{Considerations on the delta-rs library}

The system implementation focus of this project was adding support for \gls{HopsFS} to the delta-rs library. This was made possible also thanks to the built-in modularity that the library offers, having a different sub-library for each storage connector. The community around the library is very active and each question made on their communication channels, namely GitHub and Slack, would always receive an answer within a few hours or a day. The only recommendation the author would give to the library's maintainers would be to document further, perhaps with the help of architectural diagrams the inner workings of the library's processes, specifying how and when data gets uploaded into memory. The first iteration of the reading experiments had to be scratched due to calling a lazy function that would not load data into memory. 

Considering the results presented in Section \ref{sec:major_res} the similarities in performance (latency and throughput) of the two delta-rs pipelines, operating on the \gls{LocalFS} and \gls{HopsFS} respectively suggests that the library is operating at its full potential also in the newly implemented system. Delta-rs on the \gls{LocalFS} still performs faster, but this might just have to do with the different nature of the two storage systems, i.e. local and distributed.

% \subsection{delta-rs performance on \glsentryshort{HopsFS} are similar to the library's performance on the \glsentryshort{LocalFS}}

% The delta-rs on \gls{HopsFS} pipeline performs similarly (same significant figure) to the delta-rs on \gls{LocalFS} in all experiments performed on tables from 1M to 60M rows. This suggests that the developed solution takes advantage of the library's full potential. Smaller tables, i.e. 10K and 100K rows, still perform better on the \gls{LocalFS} (around 10 times better), but this should be caused by limitations given by the different nature of \gls{HopsFS}, i.e. a distributed system.

% \subsection{delta-rs outperforms the Legacy pipeline in every experiment}

% The delta-rs on \gls{HopsFS} pipeline outperforms the Legacy pipeline by a factor of ten or more in write operations and by a lower factor in read operations. While the trends suggest that with higher table dimensions there might be a cutoff between using the delta-rs pipeline and the legacy pipeline, the implemented system greatly outperforms the legacy system in the designed experiments. 

% This suggests that in the use case defined in Section \ref{subsec:use_case}, which leads to the design of the experiments, delta-rs should be adopted as the preferred solution over the legacy pipeline.

% \subsection{delta-rs scales well with increased \glsentryshort{CPU} cores}

% Both delta-rs pipelines scale considerably as the \gls{CPU} cores increase, by about 30\% (latency and throughput) in write operations and linearly for read operations (inverse for latency). On the other hand, the legacy pipeline scales poorly, with limited improvements with more \gls{CPU} cores. 

% This difference between the two systems is caused by the different architecture of the pipelines. Delta-rs performs operations (read or write) on the local machine. In contrast, the legacy pipeline uses the local machine only to upload the table in Kafka as messages (this is why the upload time improves with more \gls{CPU} cores), while it uses other computations present on a Spark cluster to perform the read or write. This means that to improve performances there is the need to give more resources to the Spark cluster, which is an aspect not controlled in these experiments (reproducing a typical scenario in the case of a Hopsworks client's deployment). 

% \subsection{The upload time in the legacy pipeline becomes the bottleneck as table size increases}

% The two steps of the legacy application writing a table show the materialization time, i.e. when Spark computes, have a great contribution to the overall latency with smaller tables, but do not grow linearly with the table sizes. On the other hand, the upload time scales linearly with the table size from the 1M table, effectively composing 95\% of the overall time for the largest table (60M rows). This is caused by how the upload process was implemented in the legacy pipeline, which iterates on each table row to upload them as Kafka messages. 

% This highlights that the Legacy pipeline while it might perform better than the implemented system (delta-rs on \gls{HopsFS}) with larger tables (more than 60 M rows), is limited in the write operation by the upload operation which scales linearly with a table size. Using a different approach to how data is uploaded in Kafka might improve performances on the legacy pipeline.