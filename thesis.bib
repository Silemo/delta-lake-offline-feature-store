@misc{ApacheDataFusionApache,
  title = {Apache {{DataFusion}} --- {{Apache DataFusion}} Documentation},
  urldate = {2024-05-21},
  howpublished = {https://datafusion.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/ZABCKKZ6/datafusion.apache.org.html}
}

@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2024-03-01},
  howpublished = {https://hadoop.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/Q39ZUKHL/hadoop.apache.org.html}
}

@misc{ApacheHudiVs,
  title = {Apache {{Hudi}} vs {{Delta Lake}} vs {{Apache Iceberg}} - {{Data Lakehouse Feature Comparison}}},
  urldate = {2024-02-28},
  abstract = {A thorough comparison of the Apache Hudi, Delta Lake, and Apache Iceberg data lakehouse projects across features, community, and performance benchmarks. This includes a focus on common use cases such as change data capture (CDC) and data ingestion.},
  howpublished = {https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison},
  file = {/Users/gio-hopsworks/Zotero/storage/EWJWIY6K/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison.html}
}

@misc{ApacheSparkUnified,
  title = {Apache {{Spark}}™ - {{Unified Engine}} for Large-Scale Data Analytics},
  urldate = {2024-03-01},
  howpublished = {https://spark.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/WLV7LSIZ/spark.apache.org.html}
}

@article{armbrustDeltaLakeHighperformance2020,
  title = {Delta Lake: High-Performance {{ACID}} Table Storage over Cloud Object Stores},
  shorttitle = {Delta Lake},
  author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and Van Hovell, Herman and Ionescu, Adrian and {\L}uszczak, Alicja and {\'S}witakowski, Micha{\l} and Szafra{\'n}ski, Micha{\l} and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {3411--3424},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415560},
  urldate = {2024-02-09},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ZNKWS6BK/Armbrust et al. - 2020 - Delta lake high-performance ACID table storage ov.pdf}
}

@misc{BenchmarkResultsSpark,
  title = {Benchmark {{Results}} for {{Spark}}, {{Dask}}, {{DuckDB}}, and {{Polars}} --- {{TPC-H Benchmarks}} at {{Scale}}},
  urldate = {2024-02-28},
  howpublished = {https://tpch.coiled.io/},
  file = {/Users/gio-hopsworks/Zotero/storage/UF64X2HH/tpch.coiled.io.html}
}

@article{carboneApacheFlinkStream,
  title = {Apache {{Flink}}™: {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KI73RD8V/Carbone et al. - Apache Flink™ Stream and Batch Processing in a Si.pdf}
}

@misc{cochranMaximizingDeveloperEffectiveness2021,
  title = {Maximizing {{Developer Effectiveness}}},
  author = {Cochran, Tim},
  year = {2021},
  month = jan,
  journal = {martinfowler.com},
  urldate = {2024-05-20},
  abstract = {To be successful at digital transformation you need to optimize the key feedback loops in your software engineering organization},
  howpublished = {https://martinfowler.com/articles/developer-effectiveness.html},
  file = {/Users/gio-hopsworks/Zotero/storage/4L6L7LQ3/developer-effectiveness.html}
}

@misc{crociDataLakehouseHype2022,
  title = {Data {{Lakehouse}}, beyond the Hype},
  author = {Croci, Daniele},
  year = {2022},
  month = dec,
  journal = {Bitrock},
  urldate = {2024-02-28},
  abstract = {In this blogpost we explore the data lakehouse as new concept that moves data lakes closer to warehouses, to compete in the BI and analytical scenario},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/84YWQSRD/data-lakehouse.html}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  urldate = {2024-02-29},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/I2KVFQJI/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@misc{DeltaioDeltars2024,
  title = {Delta-Io/Delta-Rs},
  year = {2024},
  month = may,
  urldate = {2024-05-21},
  abstract = {A native Rust library for Delta Lake, with bindings into Python},
  copyright = {Apache-2.0},
  howpublished = {Delta Lake},
  keywords = {databricks,delta,delta-lake,pandas,pandas-dataframe,python,rust}
}

@misc{DremelMadeSimple,
  title = {Dremel Made Simple with {{Parquet}}},
  urldate = {2024-02-29},
  abstract = {Dremel made simple with Parquet},
  howpublished = {https://blog.x.com/engineering/en\_us/a/2013/dremel-made-simple-with-parquet},
  langid = {american}
}

@misc{ebergenUpdatesH2OAi2023,
  title = {Updates to the {{H2O}}.Ai Db-Benchmark!},
  author = {Ebergen, Tom},
  year = {2023},
  month = nov,
  journal = {DuckDB},
  urldate = {2024-02-28},
  abstract = {The H2O.ai db-benchmark has been updated with new results. In addition, the AWS EC2 instance used for benchmarking has been changed to a c6id.metal for improved repeatability and fairness across libraries. DuckDB is the fastest library for both join and group by queries at almost every data size.},
  howpublished = {https://duckdb.org/2023/11/03/db-benchmark-update.html},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/6T3M3CNT/db-benchmark-update.html}
}

@misc{ederUnstructuredData802008,
  title = {Unstructured {{Data}} and the 80 {{Percent Rule}}},
  author = {{EDER}},
  year = {2008},
  month = aug,
  journal = {Breakthrough Analysis},
  urldate = {2024-02-29},
  abstract = {It's a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. But for all of us who cite these figures: Where did they come from? More to the{\dots}},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/3YDK8CIW/unstructured-data-and-the-80-percent-rule.html}
}

@mastersthesis{GebretsadkanKidane1413103,
  title = {Hudi on Hops : {{Incremental}} Processing and Fast Data Ingestion for Hops},
  author = {Gebretsadkan Kidane, Netsanet},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:809},
  pages = {49},
  abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi's upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Hadoop,Hops,Hudi,Kafka,Spark,SQL,Upsert},
  file = {/Users/gio-hopsworks/Zotero/storage/CCZEDP5V/Gebretsadkan Kidane - 2019 - Hudi on hops  Incremental processing and fast dat.pdf}
}

@misc{graziotinTheoryAffectSoftware2016,
  title = {Towards a {{Theory}} of {{Affect}} and {{Software Developers}}' {{Performance}}},
  author = {Graziotin, Daniel},
  year = {2016},
  month = jan,
  number = {arXiv:1601.05330},
  eprint = {1601.05330},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-20},
  abstract = {For more than thirty years, it has been claimed that a way to improve software developers' productivity and software quality is to focus on people. The underlying assumption seems to be that "happy and satisfied software developers perform better". More specifically, affects-emotions and moods-have an impact on cognitive activities and the working performance of individuals. Development tasks are undertaken heavily through cognitive processes, yet software engineering research (SE) lacks theory on affects and their impact on software development activities. This PhD dissertation supports the advocates of studying the human and social aspects of SE and the psychology of programming. This dissertation aims to theorize on the link between affects and software development performance. A mixed method approach was employed, which comprises studies of the literature in psychology and SE, quantitative experiments, and a qualitative study, for constructing a multifaceted theory of the link between affects and programming performance. The theory explicates the linkage between affects and analytical problem-solving performance of developers, their software development task productivity, and the process behind the linkage. The results are novel in the domains of SE and psychology, and they fill an important lack that had been raised by both previous research and by practitioners. The implications of this PhD lie in setting out the basic building blocks for researching and understanding the affect of software developers, and how it is related to software development performance. Overall, the evidence hints that happy software developers perform better in analytic problem solving, are more productive while developing software, are prone to share their feelings in order to let researchers and managers understand them, and are susceptible to interventions for enhancing their affects on the job.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Software Engineering,D.2.9,H.1.2,J.4},
  file = {/Users/gio-hopsworks/Zotero/storage/NSNTNDPB/Graziotin - 2016 - Towards a Theory of Affect and Software Developers.pdf;/Users/gio-hopsworks/Zotero/storage/9IEJURNG/1601.html}
}

@misc{GreenSoftwareFoundation,
  title = {Green {{Software Foundation}}},
  urldate = {2024-05-28},
  abstract = {The Green Software Foundation is a non-profit with the mission to create a trusted ecosystem of people, standards, tooling and best practices for building green software},
  howpublished = {https://greensoftware.foundation/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ZLK5KI4Q/greensoftware.foundation.html}
}

@misc{HopsworksBatchRealtime2024,
  title = {Hopsworks - {{Batch}} and {{Real-time ML Platform}}},
  year = {2024},
  urldate = {2024-02-09},
  abstract = {Hopsworks is a flexible and modular feature store that provides seamless integration for existing pipelines, superior performance for any SLA, and increased productivity for data and AI teams.},
  howpublished = {https://www.hopsworks.ai/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/UMNPVBHQ/www.hopsworks.ai.html}
}

@inproceedings{lakehouse2021,
  title = {Lakehouse: A New Generation of Open Platforms That Unify Data Warehousing and Advanced Analytics},
  booktitle = {Proceedings of {{CIDR}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  volume = {8},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  file = {/Users/gio-hopsworks/Zotero/storage/GCE6RS6H/Armbrust et al. - 2021 - Lakehouse a new generation of open platforms that.pdf}
}

@phdthesis{More862135,
  title = {{{HopsWorks}} : {{A}} Project-Based Access Control Model for {{Hadoop}}},
  author = {Mor{\'e}, Andr{\'e} and Gebremeskel, Ermias},
  year = {2015},
  series = {{{TRITA-ICT-EX}}},
  number = {2015:70},
  abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT) and KTH, School of Information and Communication Technology (ICT)},
  keywords = {Big Data,DataSets,Distributed Computing,Hadoop,Hops,HopsWorks},
  file = {/Users/gio-hopsworks/Zotero/storage/89KVT5ZU/Moré and Gebremeskel - 2015 - HopsWorks  A project-based access control model f.pdf}
}

@inproceedings{niaziHopsFSScalingHierarchical2017,
  title = {{{HopsFS}}: {{Scaling Hierarchical File System Metadata Using NewSQL Databases}}},
  shorttitle = {\{\vphantom\}{{HopsFS}}\vphantom\{\}},
  booktitle = {15th {{USENIX Conference}} on {{File}} and {{Storage Technologies}} ({{FAST}} 17)},
  author = {Niazi, Salman and Ismail, Mahmoud and Haridi, Seif and Dowling, Jim and Grohsschmiedt, Steffen and Ronstr{\"o}m, Mikael},
  year = {2017},
  pages = {89--104},
  urldate = {2024-05-21},
  isbn = {978-1-931971-36-2},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ADXXAT6R/Niazi et al. - 2017 - HopsFS Scaling Hierarchical File System Metadat.pdf}
}

@misc{Object_storeRust,
  title = {Object\_store - {{Rust}}},
  urldate = {2024-05-21},
  howpublished = {https://docs.rs/object\_store/latest/object\_store/},
  file = {/Users/gio-hopsworks/Zotero/storage/GKHSB9RH/object_store.html}
}

@mastersthesis{Pettersson1695672,
  title = {Resource-Efficient and Fast {{Point-in-Time}} Joins for {{Apache Spark}} : {{Optimization}} of Time Travel Operations for the Creation of Machine Learning Training Datasets},
  author = {Pettersson, Axel},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:193},
  pages = {63},
  abstract = {A scenario in which modern machine learning models are trained is to make use of past data to be able to make predictions about the future. When working with multiple structured and time-labeled datasets, it has become a more common practice to make use of a join operator called the Point-in-Time join, or PIT join, to construct these datasets. The PIT join matches entries from the left dataset with entries of the right dataset where the matched entry is the row whose recorded event time is the closest to the left row's timestamp, out of all the right entries whose event time occurred before or at the same time of the left event time. This feature has long only been a part of time series data processing tools but has recently received a new wave of attention due to the rise of the popularity of feature stores. To be able to perform such an operation when dealing with a large amount of data, data engineers commonly turn to large-scale data processing tools, such as Apache Spark. However, Spark does not have a native implementation when performing these joins and there has not been a clear consensus by the community on how this should be achieved. This, along with previous implementations of the PIT join, raises the question: ''How to perform fast and resource efficient Pointin- Time joins in Apache Spark?''. To answer this question, three different algorithms have been developed and compared for performing a PIT join in Spark in terms of resource consumption and execution time. These algorithms were benchmarked using generated datasets using varying physical partitions and sorting structures. Furthermore, the scalability of the algorithms was tested by running the algorithms on Apache Spark clusters of varying sizes. The results received from the benchmarks showed that the best measurements were achieved by performing the join using Early Stop Sort-Merge Join, a modified version of the regular Sort-Merge Join native to Spark. The best performing datasets were the datasets that were sorted by timestamp and primary key, ascending or descending, using a suitable number of physical partitions. Using this new information gathered by this project, data engineers have been provided with general guidelines to optimize their data processing pipelines to be able to perform more resource-efficient and faster PIT joins.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Spark,ASOF,Join,Optimeringar,Optimizations,Point-in-Time,Reading,Tidsresning,Time travel},
  file = {/Users/gio-hopsworks/Zotero/storage/8Z7JBI6G/Pettersson - 2022 - Resource-efficient and fast Point-in-Time joins fo.pdf}
}

@techreport{Python_CS-R9526,
  title = {Python Tutorial},
  author = {{van Rossum}, G.},
  year = {1995},
  month = may,
  number = {CS-R9526},
  address = {Amsterdam},
  institution = {Centrum voor Wiskunde en Informatica (CWI)},
  file = {/Users/gio-hopsworks/Zotero/storage/43WNZ76F/van Rossum - 1995 - Python tutorial.pdf}
}

@book{python-machine-learning,
  title = {Python Machine Learning (3rd Edition)},
  author = {Raschka, Vahid, Sebastian, Mirjalili},
  year = {2019},
  publisher = {Packt Publishing},
  isbn = {978-1-78995-575-0}
}

@inproceedings{raasveldtDuckDBEmbeddableAnalytical2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  pages = {1981--1984},
  publisher = {ACM},
  address = {Amsterdam Netherlands},
  doi = {10.1145/3299869.3320212},
  urldate = {2024-02-28},
  abstract = {The great popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/G72KI4R3/Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@misc{rajaperumalUberEngineeringIncremental2017,
  title = {Uber {{Engineering}}'s {{Incremental Processing Framework}} on {{Hadoop}}},
  author = {Rajaperumal, Prasanna},
  year = {2017},
  month = mar,
  journal = {Uber Blog},
  urldate = {2024-02-29},
  abstract = {Uber Engineering recently built and open sourced Hoodie, an incremental processing framework powering Uber's business critical pipelines on Hadoop.},
  howpublished = {https://www.uber.com/blog/hoodie/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/29U69TP6/hoodie.html}
}

@misc{RustProgrammingLanguage,
  title = {Rust {{Programming Language}}},
  urldate = {2024-05-21},
  abstract = {A language empowering everyone to build reliable and efficient software.},
  howpublished = {https://www.rust-lang.org/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/FBHQJHEV/www.rust-lang.org.html}
}

@techreport{StateDataLakehouse2024,
  title = {State of the {{Data Lakehouse}}},
  year = {2024},
  pages = {17},
  institution = {Dremio},
  abstract = {The data and AI technology landscape is in a constant state of rapid change. Businesses know they need to use data to compete, innovate, and succeed, but they struggle to deliver access to more of their data and to do so quickly, easily and cost-effectively. Central to meeting these challenges is a massive shift in foundational data architecture and management---the rise of the data lakehouse. Data lakehouses combine data warehouse functionality with the flexibility and scalability of data lakes, and if they are architected properly, they have the potential to improve data access, agility, and cost efficiency for analytics and AI workloads across industries. They have introduced new, streamlined data processes and have delivered value not previously available with warehouses and data lakes. This survey of 500 full-time IT and data professionals from large enterprises offers fresh insights on data lakehouse adoption and associated cost savings, open table format trends, data mesh implementation for self-service, and use of the data lakehouse in building and improving AI models and applications. The survey also explores AI's impact on jobs and the most pressing issues of the day, reflecting the unique perspectives of this highly technical cohort. The majority of respondents were IT, data, and analytics managers and directors. Data scientists, software engineers, data analysts, and data engineers also contributed to the survey results.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/Q3U89PZ2/whitepaper-2024-state-of-the-data-lakehouse_report.pdf}
}

@misc{TIOBEIndex,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2024-02-28},
  howpublished = {https://www.tiobe.com/tiobe-index/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/TKAACFMY/tiobe-index.html}
}

@misc{vinkWroteOneFastest2021,
  type = {Blog},
  title = {I Wrote One of the Fastest {{DataFrame}} Libraries},
  author = {Vink, Ritchie},
  year = {2021},
  month = feb,
  urldate = {2024-02-28},
  howpublished = {https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/},
  file = {/Users/gio-hopsworks/Zotero/storage/FIXIGF98/i-wrote-one-of-the-fastest-dataframe-libraries.html}
}

@misc{WhatGreenSoftware2021,
  title = {What Is {{Green Software}}?},
  year = {2021},
  month = oct,
  journal = {Green Software Foundation},
  urldate = {2024-05-28},
  abstract = {Creating a trusted ecosystem of people, standards, tooling, and best practices for building green software.},
  howpublished = {https://greensoftware.foundation/articles/what-is-green-software},
  file = {/Users/gio-hopsworks/Zotero/storage/ADZUW9Y4/what-is-green-software.html}
}

@techreport{Zaharia:EECS-2011-82,
  title = {Resilient {{Distributed Datasets}}: {{A Fault-Tolerant Abstraction}} for {{In-Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2011},
  month = jul,
  number = {UCB/EECS-2011-82},
  institution = {EECS Department, University of California, Berkeley},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KC5XPCLU/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf;/Users/gio-hopsworks/Zotero/storage/ZLWNKHFL/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf}
}

@article{zahariaApacheSparkUnified2016,
  title = {Apache {{Spark}}: A Unified Engine for Big Data Processing},
  shorttitle = {Apache {{Spark}}},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  year = {2016},
  month = oct,
  journal = {Communications of the ACM},
  volume = {59},
  number = {11},
  pages = {56--65},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2934664},
  urldate = {2024-02-28},
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/HD67UPVM/Zaharia et al. - 2016 - Apache Spark a unified engine for big data proces.pdf}
}
