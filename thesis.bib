@misc{hopsworks2015,
   author = {More, Andre and Gebremeskel, Ermias},
   institution = {KTH, School of Information and Communication Technology (ICT)},
   school = {KTH, School of Information and Communication Technology (ICT)},
   title = {HopsWorks : A project-based access control model for Hadoop},
   series = {TRITA-ICT-EX},
   number = {2015:70},
   keywords = {Hops, HopsWorks, Hadoop, DataSets, Big Data, Distributed Computing, Hops, HopsWorks, Hadoop, DataSets, Big Data, Distributed Computing},
   abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner. },
   year = {2015},
   url = {http://kth.diva-portal.org/smash/get/diva2:862135/FULLTEXT01.pdf},
}
@mastersthesis{ApacheHudi2019,
   author = {Gebretsadkan Kidane, Netsanet},
   institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   pages = {49},
   school = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   title = {Hudi on Hops : Incremental Processing and Fast Data Ingestion for Hops},
   series = {TRITA-EECS-EX},
   number = {2019:809},
   keywords = {Hudi, Hadoop, Hops, Upsert, SQL, Spark, Kafka, Hudi, Hadoop, Hops, Upsert, SQL, Spark, Kafka},
   abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi’s upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library. },
   year = {2019},
   url = {http://kth.diva-portal.org/smash/get/diva2:1413103/FULLTEXT01.pdf},
}
@inproceedings{lakehouse2021,
  title={Lakehouse: a new generation of open platforms that unify data warehousing and advanced analytics},
  author={Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  booktitle={Proceedings of CIDR},
  volume={8},
  year={2021},
  url = {https://15721.courses.cs.cmu.edu/spring2023/papers/02-modern/armbrust-cidr21.pdf}
}
@article{deltalake2020, 
    author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and van Hovell, Herman and Ionescu, Adrian and Luszczak, Alicja and undefinedwitakowski, Michal and Szafranski, Michal and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei}, 
    title = {Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores}, 
    year = {2020}, 
    issue_date = {August 2020}, 
    publisher = {VLDB Endowment}, 
    volume = {13}, 
    number = {12}, 
    issn = {2150-8097}, 
    url = {https://doi.org/10.14778/3415478.3415560}, 
    doi = {10.14778/3415478.3415560}, 
    abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.}, journal = {Proc. VLDB Endow.}, 
    month = {aug}, 
    pages = {3411–3424}, 
    numpages = {14} 
}

@misc{hopsworks,
	title = {Hopsworks - Batch and Real-time ML Platform},
	url = {https://www.hopsworks.ai},
}