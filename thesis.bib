@inproceedings{10.1145/3626246.3653389,
  title = {The Hopsworks Feature Store for Machine Learning},
  booktitle = {Companion of the 2024 International Conference on Management of Data},
  author = {{de la R{\'u}a Mart{\'{\i}}nez}, Javier and Buso, Fabio and Kouzoupis, Antonios and Ormenisan, Alexandru A. and Niazi, Salman and Bzhalava, Davit and Mak, Kenneth and Jouffrey, Victor and Ronstr{\"o}m, Mikael and Cunningham, Raymond and Zangis, Ralfs and Mukhedkar, Dhananjay and Khazanchi, Ayushman and Vlassov, Vladimir and Dowling, Jim},
  year = {2024},
  series = {Sigmod/Pods '24},
  pages = {135--147},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3626246.3653389},
  abstract = {Data management is the most challenging aspect of building Machine Learning (ML) systems. ML systems can read large volumes of historical data when training models, but inference workloads are more varied, depending on whether it is a batch or online ML system. The feature store for ML has recently emerged as a single data platform for managing ML data throughout the ML lifecycle, from feature engineering to model training to inference. In this paper, we present the Hopsworks feature store for machine learning as a highly available platform for managing feature data with API support for columnar, row-oriented, and similarity search query workloads. We introduce and address challenges solved by the feature stores related to feature reuse, how to organize data transformations, and how to ensure correct and consistent data between feature engineering, model training, and model inference. We present the engineering challenges in building high-performance query services for a feature store and show how Hopsworks outperforms existing cloud feature stores for training and online inference query workloads.},
  isbn = {9798400704222},
  keywords = {arrow flight,duckdb,feature store,mlops,rondb},
  file = {/Users/gio-hopsworks/Zotero/storage/ZS9HBS58/de la Rúa Martı́nez et al. - 2024 - The hopsworks feature store for machine learning.pdf}
}

@misc{altexsoftHowDataEngineering2021,
  title = {How {{Data Engineering Works}} - {{Youtube}}},
  author = {AltexSoft},
  year = {2021},
  urldate = {2024-06-06},
  howpublished = {https://www.youtube.com/watch?v=qWru-b6m030\&t=485s},
  file = {/Users/gio-hopsworks/Zotero/storage/NI2ITYRT/watch.html}
}

@misc{ApacheArrowrs2024,
  title = {Apache/Arrow-Rs},
  year = {2024},
  month = sep,
  urldate = {2024-09-11},
  abstract = {Official Rust implementation of Apache Arrow},
  copyright = {Apache-2.0},
  howpublished = {The Apache Software Foundation}
}

@misc{ApacheDataFusionApache,
  title = {Apache {{DataFusion}} --- {{Apache DataFusion}} Documentation},
  urldate = {2024-05-21},
  howpublished = {https://datafusion.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/ZABCKKZ6/datafusion.apache.org.html}
}

@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2024-03-01},
  howpublished = {https://hadoop.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/Q39ZUKHL/hadoop.apache.org.html}
}

@misc{ApacheHudiVs,
  title = {Apache {{Hudi}} vs {{Delta Lake}} vs {{Apache Iceberg}} - {{Data Lakehouse Feature Comparison}}},
  urldate = {2024-02-28},
  abstract = {A thorough comparison of the Apache Hudi, Delta Lake, and Apache Iceberg data lakehouse projects across features, community, and performance benchmarks. This includes a focus on common use cases such as change data capture (CDC) and data ingestion.},
  howpublished = {https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison},
  file = {/Users/gio-hopsworks/Zotero/storage/EWJWIY6K/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison.html}
}

@misc{ApacheSparkOpen,
  title = {The {{Apache Spark Open Source Project}} on {{Open Hub}}},
  urldate = {2024-07-19},
  howpublished = {https://openhub.net/p/apache-spark},
  file = {/Users/gio-hopsworks/Zotero/storage/HGCI67WB/apache-spark.html}
}

@misc{ApacheSparkUnified,
  title = {Apache {{Spark}}™ - {{Unified Engine}} for Large-Scale Data Analytics},
  urldate = {2024-03-01},
  howpublished = {https://spark.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/WLV7LSIZ/spark.apache.org.html}
}

@article{armbrustDeltaLakeHighperformance2020,
  title = {Delta Lake: High-Performance {{ACID}} Table Storage over Cloud Object Stores},
  shorttitle = {Delta Lake},
  author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and Van Hovell, Herman and Ionescu, Adrian and {\L}uszczak, Alicja and {\'S}witakowski, Micha{\l} and Szafra{\'n}ski, Micha{\l} and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {3411--3424},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415560},
  urldate = {2024-02-09},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ZNKWS6BK/Armbrust et al. - 2020 - Delta lake high-performance ACID table storage ov.pdf}
}

@misc{ArrowrsObject_storeREADMEmd,
  title = {Arrow-Rs/Object\_store/{{README}}.Md at Master {$\cdot$} Apache/Arrow-Rs},
  journal = {GitHub},
  urldate = {2024-09-24},
  abstract = {Official Rust implementation of Apache Arrow. Contribute to apache/arrow-rs development by creating an account on GitHub.},
  howpublished = {https://github.com/apache/arrow-rs/blob/master/object\_store/README.md},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/SDMZY9XP/object_store.html}
}

@inproceedings{behmPhotonFastQuery2022,
  title = {Photon: {{A Fast Query Engine}} for {{Lakehouse Systems}}},
  shorttitle = {Photon},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Behm, Alexander and Palkar, Shoumik and Agarwal, Utkarsh and Armstrong, Timothy and Cashman, David and Dave, Ankur and Greenstein, Todd and Hovsepian, Shant and Johnson, Ryan and Sai Krishnan, Arvind and Leventis, Paul and Luszczak, Ala and Menon, Prashanth and Mokhtar, Mostafa and Pang, Gene and Paranjpye, Sameer and Rahn, Greg and Samwel, Bart and Van Bussel, Tom and Van Hovell, Herman and Xue, Maryann and Xin, Reynold and Zaharia, Matei},
  year = {2022},
  month = jun,
  pages = {2326--2339},
  publisher = {ACM},
  address = {Philadelphia PA USA},
  doi = {10.1145/3514221.3526054},
  urldate = {2024-09-19},
  abstract = {Many organizations are shifting to a data management paradigm called the ``Lakehouse,'' which implements the functionality of structured data warehouses on top of unstructured data lakes. This presents new challenges for query execution engines. The execution engine needs to provide good performance on the raw uncurated datasets that are ubiquitous in data lakes, and excellent performance on structured data stored in popular columnar file formats like Apache Parquet. Toward these goals, we present Photon, a vectorized query engine for Lakehouse environments that we developed at Databricks. Photon can outperform existing cloud data warehouses in SQL workloads, but implements a more general execution framework that enables efficient processing of raw data and also enables Photon to support the Apache Spark API. We discuss the design choices we made in Photon (e.g., vectorization vs. code generation) and describe its integration with our existing SQL and Apache Spark runtimes, its task model, and its memory manager. Photon has accelerated some customer workloads by over 10{\texttimes} and has recently allowed Databricks to set a new audited performance record for the official 100TB TPC-DS benchmark.},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/945EMJAH/Behm et al. - 2022 - Photon A Fast Query Engine for Lakehouse Systems.pdf}
}

@misc{BenchmarkResultsSpark,
  title = {Benchmark {{Results}} for {{Spark}}, {{Dask}}, {{DuckDB}}, and {{Polars}} --- {{TPC-H Benchmarks}} at {{Scale}}},
  urldate = {2024-02-28},
  howpublished = {https://tpch.coiled.io/},
  file = {/Users/gio-hopsworks/Zotero/storage/UF64X2HH/tpch.coiled.io.html}
}

@misc{binfordKimahrimanHdfsnative2024,
  title = {Kimahriman/Hdfs-Native},
  author = {Binford, Adam},
  year = {2024},
  month = aug,
  urldate = {2024-09-25},
  copyright = {Apache-2.0}
}

@misc{BlockVsFile,
  title = {Block vs {{File}} vs {{Object Storage}} - {{Difference Between Data Storage Services}} - {{AWS}}},
  journal = {Amazon Web Services, Inc.},
  urldate = {2024-07-23},
  abstract = {What's the difference between Block, File and Object Storage? How to Use Block, File and Object storage with AWS.},
  howpublished = {https://aws.amazon.com/compare/the-difference-between-block-file-object-storage/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/ZI6H6GBG/the-difference-between-block-file-object-storage.html}
}

@article{borthakurHadoopDistributedFile2005,
  title = {The {{Hadoop Distributed File System}}: {{Architecture}} and {{Design}}},
  author = {Borthakur, Dhruba},
  year = {2005},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/IZE29H8Y/Borthakur - 2005 - The Hadoop Distributed File System Architecture a.pdf}
}

@article{carboneApacheFlinkStream,
  title = {Apache {{Flink}}™: {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KI73RD8V/Carbone et al. - Apache Flink™ Stream and Batch Processing in a Si.pdf}
}

@article{chaudhuriOverviewDataWarehousing1997,
  title = {An Overview of Data Warehousing and {{OLAP}} Technology},
  author = {Chaudhuri, Surajit and Dayal, Umeshwar},
  year = {1997},
  month = mar,
  journal = {ACM SIGMOD Record},
  volume = {26},
  number = {1},
  pages = {65--74},
  issn = {0163-5808},
  doi = {10.1145/248603.248616},
  urldate = {2024-06-14},
  abstract = {Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/M9GNTRHN/Chaudhuri and Dayal - 1997 - An overview of data warehousing and OLAP technolog.pdf}
}

@misc{cochranMaximizingDeveloperEffectiveness2021,
  title = {Maximizing {{Developer Effectiveness}}},
  author = {Cochran, Tim},
  year = {2021},
  month = jan,
  journal = {martinfowler.com},
  urldate = {2024-05-20},
  abstract = {To be successful at digital transformation you need to optimize the key feedback loops in your software engineering organization},
  howpublished = {https://martinfowler.com/articles/developer-effectiveness.html},
  file = {/Users/gio-hopsworks/Zotero/storage/4L6L7LQ3/developer-effectiveness.html}
}

@misc{crociDataLakehouseHype2022,
  title = {Data {{Lakehouse}}, beyond the Hype},
  author = {Croci, Daniele},
  year = {2022},
  month = dec,
  journal = {Bitrock},
  urldate = {2024-02-28},
  abstract = {In this blogpost we explore the data lakehouse as new concept that moves data lakes closer to warehouses, to compete in the BI and analytical scenario},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/84YWQSRD/data-lakehouse.html}
}

@inproceedings{dean2004mapreduce,
  title = {{{MapReduce}}: {{Simplified}} Data Processing on Large Clusters},
  booktitle = {{{OSDI}}'04: {{Sixth}} Symposium on Operating System Design and Implementation},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2004},
  pages = {137--150},
  address = {San Francisco, CA}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  urldate = {2024-02-29},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/I2KVFQJI/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@misc{DeltaioDeltars2024,
  title = {Delta-Io/Delta-Rs},
  year = {2024},
  month = may,
  urldate = {2024-05-21},
  abstract = {A native Rust library for Delta Lake, with bindings into Python},
  copyright = {Apache-2.0},
  howpublished = {Delta Lake},
  keywords = {databricks,delta,delta-lake,pandas,pandas-dataframe,python,rust}
}

@misc{DeltarsPythonMain,
  title = {Delta-Rs/Python at Main {$\cdot$} {{Silemo}}/Delta-Rs},
  urldate = {2024-09-25},
  howpublished = {https://github.com/Silemo/delta-rs/tree/main/python},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/RS7LK3VI/python.html}
}

@article{despa2014comparative,
  title = {Comparative Study on Software Development Methodologies.},
  author = {Despa, Mihai Liviu},
  year = {2014},
  journal = {Database Systems Journal},
  volume = {5},
  number = {3}
}

@misc{DockerBuild0200,
  title = {Docker {{Build}}},
  year = {12:14:28 +0200 +0200},
  journal = {Docker Documentation},
  urldate = {2024-09-11},
  abstract = {Get an overview of Docker Build to package and bundle your code and ship it anywhere},
  howpublished = {https://docs.docker.com/build/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/J5Q4VKNU/build.html}
}

@misc{DremelMadeSimple,
  title = {Dremel Made Simple with {{Parquet}}},
  urldate = {2024-02-29},
  abstract = {Dremel made simple with Parquet},
  howpublished = {https://blog.x.com/engineering/en\_us/a/2013/dremel-made-simple-with-parquet},
  langid = {american}
}

@misc{ebergenUpdatesH2OAi2023,
  title = {Updates to the {{H2O}}.Ai Db-Benchmark!},
  author = {Ebergen, Tom},
  year = {2023},
  month = nov,
  journal = {DuckDB},
  urldate = {2024-02-28},
  abstract = {The H2O.ai db-benchmark has been updated with new results. In addition, the AWS EC2 instance used for benchmarking has been changed to a c6id.metal for improved repeatability and fairness across libraries. DuckDB is the fastest library for both join and group by queries at almost every data size.},
  howpublished = {https://duckdb.org/2023/11/03/db-benchmark-update.html},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/6T3M3CNT/db-benchmark-update.html}
}

@misc{ederUnstructuredData802008,
  title = {Unstructured {{Data}} and the 80 {{Percent Rule}}},
  author = {{EDER}},
  year = {2008},
  month = aug,
  journal = {Breakthrough Analysis},
  urldate = {2024-02-29},
  abstract = {It's a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. But for all of us who cite these figures: Where did they come from? More to the{\dots}},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/3YDK8CIW/unstructured-data-and-the-80-percent-rule.html}
}

@misc{fannFannheywardCocrustanalyzer2024,
  title = {Fannheyward/Coc-Rust-Analyzer},
  author = {Fann, Heyward},
  year = {2024},
  month = sep,
  urldate = {2024-09-11},
  abstract = {rust-analyzer extension for coc.nvim},
  copyright = {MIT},
  keywords = {coc,coc-extensions,coc-nvim,rust,rust-analyzer}
}

@book{framptonCompleteGuideOpen2018,
  title = {Complete {{Guide}} to {{Open Source Big Data Stack}}},
  author = {Frampton, Michael},
  year = {2018},
  month = jan,
  urldate = {2024-07-23},
  abstract = {See a Mesos-based big data stack created and the components used. You will use currently available Apache full and incubating systems. The components are introduced by example and you learn how...},
  isbn = {978-1-4842-2149-5},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/AP79WZ8E/9781484221495.html}
}

@mastersthesis{GebretsadkanKidane1413103,
  title = {Hudi on Hops : {{Incremental}} Processing and Fast Data Ingestion for Hops},
  author = {Gebretsadkan Kidane, Netsanet},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:809},
  pages = {49},
  abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi's upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Hadoop,Hops,Hudi,Kafka,Spark,SQL,Upsert},
  file = {/Users/gio-hopsworks/Zotero/storage/CCZEDP5V/Gebretsadkan Kidane - 2019 - Hudi on hops  Incremental processing and fast dat.pdf}
}

@misc{GitHub,
  title = {{{GitHub}}},
  journal = {GitHub},
  urldate = {2024-09-11},
  abstract = {GitHub is where people build software. More than 100 million people use GitHub to discover, fork, and contribute to over 420 million projects.},
  howpublished = {https://github.com},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/3GNCABXF/github.com.html}
}

@article{gortonDistributionDataDeployment2015,
  title = {Distribution, {{Data}}, {{Deployment}}: {{Software Architecture Convergence}} in {{Big Data Systems}}},
  shorttitle = {Distribution, {{Data}}, {{Deployment}}},
  author = {Gorton, Ian and Klein, John},
  year = {2015},
  month = may,
  journal = {IEEE Software},
  volume = {32},
  number = {3},
  pages = {78--85},
  issn = {1937-4194},
  doi = {10.1109/MS.2014.51},
  urldate = {2024-05-30},
  abstract = {Big data applications are pushing the limits of software engineering on multiple horizons. Successful solutions span the design of the data, distribution, and deployment architectures. The body of software architecture knowledge must evolve to capture this advanced design knowledge for big data systems. This article is a first step on this path. Our research is proceeding in two complementary directions. First, we're expanding our collection of architecture tactics and encoding them in an environment that supports navigation between quality attributes and tactics, making crosscutting concerns for design choices explicit. Second, we're linking tactics to design solutions based on specific big data technologies, enabling architects to rapidly relate a particular technology's capabilities to a specific set of tactics.},
  keywords = {big data,Big data,Computer architecture,data management,Data management,Data models,Distributed databases,distributed systems,NoSQL,software architecture,Software architecture,software engineering,Software engineering},
  file = {/Users/gio-hopsworks/Zotero/storage/MN4MW9BZ/6774768.html}
}

@misc{graziotinTheoryAffectSoftware2016,
  title = {Towards a {{Theory}} of {{Affect}} and {{Software Developers}}' {{Performance}}},
  author = {Graziotin, Daniel},
  year = {2016},
  month = jan,
  number = {arXiv:1601.05330},
  eprint = {1601.05330},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-20},
  abstract = {For more than thirty years, it has been claimed that a way to improve software developers' productivity and software quality is to focus on people. The underlying assumption seems to be that "happy and satisfied software developers perform better". More specifically, affects-emotions and moods-have an impact on cognitive activities and the working performance of individuals. Development tasks are undertaken heavily through cognitive processes, yet software engineering research (SE) lacks theory on affects and their impact on software development activities. This PhD dissertation supports the advocates of studying the human and social aspects of SE and the psychology of programming. This dissertation aims to theorize on the link between affects and software development performance. A mixed method approach was employed, which comprises studies of the literature in psychology and SE, quantitative experiments, and a qualitative study, for constructing a multifaceted theory of the link between affects and programming performance. The theory explicates the linkage between affects and analytical problem-solving performance of developers, their software development task productivity, and the process behind the linkage. The results are novel in the domains of SE and psychology, and they fill an important lack that had been raised by both previous research and by practitioners. The implications of this PhD lie in setting out the basic building blocks for researching and understanding the affect of software developers, and how it is related to software development performance. Overall, the evidence hints that happy software developers perform better in analytic problem solving, are more productive while developing software, are prone to share their feelings in order to let researchers and managers understand them, and are susceptible to interventions for enhancing their affects on the job.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Software Engineering,D.2.9,H.1.2,J.4},
  file = {/Users/gio-hopsworks/Zotero/storage/NSNTNDPB/Graziotin - 2016 - Towards a Theory of Affect and Software Developers.pdf;/Users/gio-hopsworks/Zotero/storage/9IEJURNG/1601.html}
}

@misc{GreenSoftwareFoundation,
  title = {Green {{Software Foundation}}},
  urldate = {2024-05-28},
  abstract = {The Green Software Foundation is a non-profit with the mission to create a trusted ecosystem of people, standards, tooling and best practices for building green software},
  howpublished = {https://greensoftware.foundation/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ZLK5KI4Q/greensoftware.foundation.html}
}

@misc{HopsworksBatchRealtime2024,
  title = {Hopsworks - {{Batch}} and {{Real-time ML Platform}}},
  year = {2024},
  urldate = {2024-02-09},
  abstract = {Hopsworks is a flexible and modular feature store that provides seamless integration for existing pipelines, superior performance for any SLA, and increased productivity for data and AI teams.},
  howpublished = {https://www.hopsworks.ai/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/UMNPVBHQ/www.hopsworks.ai.html}
}

@misc{HowObjectVs,
  title = {How {{Object}} vs {{Block}} vs {{File Storage}} Differ},
  journal = {Google Cloud},
  urldate = {2024-07-23},
  abstract = {Learn the differences between object, file, and block storage including what data they store and how they store it.},
  howpublished = {https://cloud.google.com/discover/object-vs-block-vs-file-storage},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/AIIQ85PI/object-vs-block-vs-file-storage.html}
}

@mastersthesis{Khazanchi1801362,
  title = {Faster Reading with {{DuckDB}} and Arrow Flight on Hopsworks : {{Benchmark}} and Performance Evaluation of Offline Feature Stores},
  author = {Khazanchi, Ayushman},
  year = {2023},
  series = {{{TRITA-EECS-EX}}},
  number = {2023:661},
  pages = {69},
  abstract = {Over the last few years, Machine Learning has become a huge field with ``Big Tech'' companies sharing their experiences building machine learning infrastructure. Feature Stores, used as centralized data repositories for machine learning features, are seen as a central component to operational and scalable machine learning. With the growth in machine learning, there is, naturally, a tremendous growth in data used for training. Most of this data tends to sit in Parquet files in cloud object stores or data lakes and is used either directly from files or in-memory where it is used in exploratory data analysis and small batches of training. A majority of the data science involved in machine learning is done in Python, but the infrastructure surrounding it is not always directly compatible with Python. Often, query processing engines and feature stores end up having their own Domain Specific Language or require data scientists to write SQL code, thus leading to some level of `transpilation' overhead across the system. This overhead can not only introduce errors but can also add up to significant time and productivity cost down the line. In this thesis, we conduct a systems research on the performance of offline feature stores and identify ways that allow us to pull out data from feature stores in a fast and efficient way. We conduct a model evaluation based on benchmark tests that address common exploratory data analysis and training use cases. We find that in the Hopsworks feature store, with the use of state-of-the-art, storage-optimized, format-aware, and vector execution-based query processing engine as well as using Arrow protocol from start to finish, we are able to see significant improvements in both creating batch training data (feature value reads) and creating Point-In-Time Correct training data. For batch training data created in-memory, Hopsworks shows an average speedup of 27x over Databricks (5M and 10M scale factors), 18x over Vertex, and 8x over Sagemaker across all scale factors. For batch training data as parquet files, Hopsworks shows a speedup of 5x over Databricks (5M, 10M, and 20M scale factors), 13x over Vertex, and 6x over Sagemaker across all scale factors. For creating in-memory Point-In-Time Correct training data, Hopsworks shows an average speedup of 8x over Databricks, 6x over Vertex, and 3x over Sagemaker across all scale factors. Similary for PIT-Correct training data created as file, Hopsworks shows an average speedup of 9x over Databricks, 8x over Vertex, and 6x over Sagemaker across all scale factors. Through the analysis of these experimental results and the underlying infrastructure, we identify the reasons for this performance gap and examine the strengths and limitations of the design.},
  school = {KTH Royal Institute of Technology / KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Distributed Systems,Feature Store,Machine Learning,MLOps},
  file = {/Users/gio-hopsworks/Zotero/storage/KFAWM2A6/Khazanchi - 2023 - Faster reading with DuckDB and arrow flight on hop.pdf}
}

@inproceedings{lakehouse2021,
  title = {Lakehouse: A New Generation of Open Platforms That Unify Data Warehousing and Advanced Analytics},
  booktitle = {Proceedings of {{CIDR}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  volume = {8},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  file = {/Users/gio-hopsworks/Zotero/storage/GCE6RS6H/Armbrust et al. - 2021 - Lakehouse a new generation of open platforms that.pdf}
}

@misc{manfrediSilemoDeltars2024,
  title = {Silemo/Delta-Rs},
  author = {Manfredi, Giovanni},
  year = {2024},
  month = sep,
  urldate = {2024-09-25},
  abstract = {A native Rust library for Delta Lake, with bindings into Python},
  copyright = {Apache-2.0}
}

@misc{manfrediSilemoHdfsnative2024,
  title = {Silemo/Hdfs-Native},
  author = {Manfredi, Giovanni},
  year = {2024},
  month = aug,
  urldate = {2024-09-25},
  copyright = {Apache-2.0}
}

@misc{manfrediSilemoHdfsnativeobjectstore2024,
  title = {Silemo/Hdfs-Native-Object-Store},
  author = {Manfredi, Giovanni},
  year = {2024},
  month = aug,
  urldate = {2024-09-25},
  abstract = {Native rust implementation of object\_store HDFS},
  copyright = {Apache-2.0}
}

@phdthesis{More862135,
  title = {{{HopsWorks}} : {{A}} Project-Based Access Control Model for {{Hadoop}}},
  author = {Mor{\'e}, Andr{\'e} and Gebremeskel, Ermias},
  year = {2015},
  series = {{{TRITA-ICT-EX}}},
  number = {2015:70},
  abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT) and KTH, School of Information and Communication Technology (ICT)},
  keywords = {Big Data,DataSets,Distributed Computing,Hadoop,Hops,HopsWorks},
  file = {/Users/gio-hopsworks/Zotero/storage/89KVT5ZU/Moré and Gebremeskel - 2015 - HopsWorks  A project-based access control model f.pdf}
}

@inproceedings{nagpalPythonDataAnalytics2019,
  title = {Python for {{Data Analytics}}, {{Scientific}} and {{Technical Applications}}},
  booktitle = {2019 {{Amity International Conference}} on {{Artificial Intelligence}} ({{AICAI}})},
  author = {Nagpal, Abhinav and Gabrani, Goldie},
  year = {2019},
  month = feb,
  pages = {140--145},
  doi = {10.1109/AICAI.2019.8701341},
  urldate = {2024-06-14},
  abstract = {Since the invention of computers or machines, their capability to perform various tasks has experienced an exponential growth. In the current times, data science and analytics, a branch of computer science, has revived due to the major increase in computer power, presence of huge amounts of data, and better understanding in techniques in the area of Data Analytics, Artificial Intelligence, Machine Learning, Deep Learning etc. Hence, they have become an essential part of the technology industry, and are being used to solve many challenging problems. In the search for a good programming language on which many data science applications can be developed, python has emerged as a complete programming solution. Due to the low learning curve, and flexibility of Python, it has become one of the fastest growing languages. Python's ever-evolving libraries make it a good choice for Data analytics. The paper talks about the features and characteristics of Python programming language and later discusses reasons behind python being credited as one of the fastest growing programming language and why it is at the forefront of data science applications, research and development.},
  keywords = {Artificial intelligence,Artificial Intelligence,Computer Languages,Data analysis,Data Analytics,Deep Learning,Frameworks,Libraries,Machine Learning,Matlab,Natural Language Processing,Python,Scientific Computations,Tools},
  file = {/Users/gio-hopsworks/Zotero/storage/LQLH435Z/8701341.html}
}

@misc{NeoclideCocnvim2024,
  title = {Neoclide/Coc.Nvim},
  year = {2024},
  month = sep,
  urldate = {2024-09-11},
  abstract = {Nodejs extension host for vim \& neovim, load extensions like VSCode and host language servers.},
  howpublished = {Neoclide},
  keywords = {autocompletion,language-client,lsp,neovim-plugin,nvim,vim,vim-plugin}
}

@inproceedings{niaziHopsFSScalingHierarchical2017,
  title = {{{HopsFS}}: {{Scaling Hierarchical File System Metadata Using NewSQL Databases}}},
  shorttitle = {\{\vphantom\}{{HopsFS}}\vphantom\{\}},
  booktitle = {15th {{USENIX Conference}} on {{File}} and {{Storage Technologies}} ({{FAST}} 17)},
  author = {Niazi, Salman and Ismail, Mahmoud and Haridi, Seif and Dowling, Jim and Grohsschmiedt, Steffen and Ronstr{\"o}m, Mikael},
  year = {2017},
  pages = {89--104},
  urldate = {2024-05-21},
  isbn = {978-1-931971-36-2},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ADXXAT6R/Niazi et al. - 2017 - HopsFS Scaling Hierarchical File System Metadat.pdf}
}

@misc{Object_storeRust,
  title = {Object\_store - {{Rust}}},
  urldate = {2024-05-21},
  howpublished = {https://docs.rs/object\_store/latest/object\_store/},
  file = {/Users/gio-hopsworks/Zotero/storage/GKHSB9RH/object_store.html}
}

@misc{ObjectVsFile2021,
  title = {Object vs. {{File}} vs. {{Block Storage}}: {{What}}'s the {{Difference}}?},
  shorttitle = {Object vs. {{File}} vs. {{Block Storage}}},
  year = {2021},
  month = oct,
  journal = {IBM Blog},
  urldate = {2024-07-23},
  abstract = {A helpful look into file, object and block storage, their key differences and what type best meets your needs.},
  howpublished = {https://www.ibm.com/blog/object-vs-file-vs-block-storage/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/GGY3NPZ5/object-vs-file-vs-block-storage.html}
}

@misc{pattersonCarbonEmissionsLarge2021,
  title = {Carbon {{Emissions}} and {{Large Neural Network Training}}},
  author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2104.10350},
  urldate = {2024-07-19},
  abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume \&lt;1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computers and Society (cs.CY),FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@misc{pattersonCarbonFootprintMachine2022,
  title = {The {{Carbon Footprint}} of {{Machine Learning Training Will Plateau}}, {{Then Shrink}}},
  author = {Patterson, David and Gonzalez, Joseph and H{\"o}lzle, Urs and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2204.05149},
  urldate = {2024-07-19},
  abstract = {Machine Learning (ML) workloads have rapidly grown in importance, but raised concerns about their carbon footprint. Four best practices can reduce ML training energy by up to 100x and CO2 emissions up to 1000x. By following best practices, overall ML energy use (across research, development, and production) held steady at \&lt;15\% of Google's total energy use for the past three years. If the whole ML field were to adopt best practices, total carbon emissions from training would reduce. Hence, we recommend that ML papers include emissions explicitly to foster competition on more than just model quality. Estimates of emissions in papers that omitted them have been off 100x-100,000x, so publishing emissions has the added benefit of ensuring accurate accounting. Given the importance of climate change, we must get the numbers right to make certain that we work on its biggest challenges.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Artificial Intelligence (cs.AI),FOS: Computer and information sciences,General Literature (cs.GL),Machine Learning (cs.LG)}
}

@article{penceWhatBigData2014,
  title = {What Is {{Big Data}} and {{Why}} Is It {{Important}}?},
  author = {Pence, Harry E.},
  year = {2014},
  month = dec,
  journal = {Journal of Educational Technology Systems},
  volume = {43},
  number = {2},
  pages = {159--171},
  issn = {0047-2395, 1541-3810},
  doi = {10.2190/ET.43.2.d},
  urldate = {2024-05-30},
  abstract = {Big Data Analytics is a topic fraught with both positive and negative potential. Big Data is defined not just by the amount of information involved but also its variety and complexity, as well as the speed with which it must be analyzed or delivered. The amount of data being produced is already incredibly great, and current developments suggest that this rate will only increase in the near future. Improved service should result as companies better understand their customers, but it is also possible that this data will create privacy problems. Thus, Big Data is important not only to students who hope to gain employment using these techniques and those who plan to use it for legitimate research, but also for everyone who will be living and working in the 21st Century.},
  langid = {english}
}

@mastersthesis{Pettersson1695672,
  title = {Resource-Efficient and Fast {{Point-in-Time}} Joins for {{Apache Spark}} : {{Optimization}} of Time Travel Operations for the Creation of Machine Learning Training Datasets},
  author = {Pettersson, Axel},
  year = {2022},
  series = {{{TRITA-EECS-EX}}},
  number = {2022:193},
  pages = {63},
  abstract = {A scenario in which modern machine learning models are trained is to make use of past data to be able to make predictions about the future. When working with multiple structured and time-labeled datasets, it has become a more common practice to make use of a join operator called the Point-in-Time join, or PIT join, to construct these datasets. The PIT join matches entries from the left dataset with entries of the right dataset where the matched entry is the row whose recorded event time is the closest to the left row's timestamp, out of all the right entries whose event time occurred before or at the same time of the left event time. This feature has long only been a part of time series data processing tools but has recently received a new wave of attention due to the rise of the popularity of feature stores. To be able to perform such an operation when dealing with a large amount of data, data engineers commonly turn to large-scale data processing tools, such as Apache Spark. However, Spark does not have a native implementation when performing these joins and there has not been a clear consensus by the community on how this should be achieved. This, along with previous implementations of the PIT join, raises the question: ''How to perform fast and resource efficient Pointin- Time joins in Apache Spark?''. To answer this question, three different algorithms have been developed and compared for performing a PIT join in Spark in terms of resource consumption and execution time. These algorithms were benchmarked using generated datasets using varying physical partitions and sorting structures. Furthermore, the scalability of the algorithms was tested by running the algorithms on Apache Spark clusters of varying sizes. The results received from the benchmarks showed that the best measurements were achieved by performing the join using Early Stop Sort-Merge Join, a modified version of the regular Sort-Merge Join native to Spark. The best performing datasets were the datasets that were sorted by timestamp and primary key, ascending or descending, using a suitable number of physical partitions. Using this new information gathered by this project, data engineers have been provided with general guidelines to optimize their data processing pipelines to be able to perform more resource-efficient and faster PIT joins.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Apache Spark,ASOF,Join,Optimeringar,Optimizations,Point-in-Time,Reading,Tidsresning,Time travel},
  file = {/Users/gio-hopsworks/Zotero/storage/8Z7JBI6G/Pettersson - 2022 - Resource-efficient and fast Point-in-Time joins fo.pdf}
}

@techreport{Python_CS-R9526,
  title = {Python Tutorial},
  author = {{van Rossum}, G.},
  year = {1995},
  month = may,
  number = {CS-R9526},
  address = {Amsterdam},
  institution = {Centrum voor Wiskunde en Informatica (CWI)},
  file = {/Users/gio-hopsworks/Zotero/storage/43WNZ76F/van Rossum - 1995 - Python tutorial.pdf}
}

@book{python-machine-learning,
  title = {Python Machine Learning (3rd Edition)},
  author = {Raschka, Mirjalili and Vahid, Sebastian},
  year = {2019},
  publisher = {Packt Publishing},
  isbn = {978-1-78995-575-0}
}

@inproceedings{raasveldtDuckDBEmbeddableAnalytical2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  pages = {1981--1984},
  publisher = {ACM},
  address = {Amsterdam Netherlands},
  doi = {10.1145/3299869.3320212},
  urldate = {2024-02-28},
  abstract = {The great popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/G72KI4R3/Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@misc{rajaperumalUberEngineeringIncremental2017,
  title = {Uber {{Engineering}}'s {{Incremental Processing Framework}} on {{Hadoop}}},
  author = {Rajaperumal, Prasanna},
  year = {2017},
  month = mar,
  journal = {Uber Blog},
  urldate = {2024-02-29},
  abstract = {Uber Engineering recently built and open sourced Hoodie, an incremental processing framework powering Uber's business critical pipelines on Hadoop.},
  howpublished = {https://www.uber.com/blog/hoodie/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/29U69TP6/hoodie.html}
}

@misc{RustlsTokiorustlsAsync,
  title = {Rustls/Tokio-Rustls: {{Async TLS}} for the {{Tokio}} Runtime},
  shorttitle = {Rustls/Tokio-Rustls},
  urldate = {2024-09-25},
  howpublished = {https://github.com/rustls/tokio-rustls},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/XQN5KYTV/main.html}
}

@misc{RustProgrammingLanguage,
  title = {Rust {{Programming Language}}},
  urldate = {2024-05-21},
  abstract = {A language empowering everyone to build reliable and efficient software.},
  howpublished = {https://www.rust-lang.org/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/FBHQJHEV/www.rust-lang.org.html}
}

@article{sakrBigDataProcessing2017,
  title = {Big {{Data Processing Stacks}}},
  author = {Sakr, Sherif},
  year = {2017},
  month = jan,
  journal = {IT Professional},
  volume = {19},
  number = {1},
  pages = {34--41},
  issn = {1941-045X},
  doi = {10.1109/MITP.2017.6},
  urldate = {2024-07-23},
  abstract = {After roughly a decade of dominance by the Hadoop framework in the Big Data processing world, we are witnessing the emergence of various stacks that have been enhanced with domain-specific, optimized, and vertically focused Big Data processing features. The author analyzes in detail the capabilities of various Big Data processing stacks and provides insights and guidelines about the latest ongoing developments in this domain.},
  keywords = {big data,Big data,Biological system modeling,Computational modeling,data analysis,Distributed databases,Flink,Hadoop,Programming,Spark,Sparks,Structured Query Language},
  file = {/Users/gio-hopsworks/Zotero/storage/53PMAB5R/Sakr - 2017 - Big Data Processing Stacks.pdf;/Users/gio-hopsworks/Zotero/storage/D4XRJNBP/7839846.html}
}

@misc{StackOverflowDeveloper,
  title = {Stack {{Overflow Developer Survey}} 2023},
  journal = {Stack Overflow},
  urldate = {2024-06-14},
  abstract = {In May 2023 over 90,000 developers responded to our annual survey about how they learn and level up, which tools they're using, and which ones they want.},
  howpublished = {https://survey.stackoverflow.co/2023/?utm\_source=social-share\&utm\_medium=social\&utm\_campaign=dev-survey-2023},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/26QZ39NL/2023.html}
}

@techreport{StateDataLakehouse2024,
  title = {State of the {{Data Lakehouse}}},
  year = {2024},
  pages = {17},
  institution = {Dremio},
  abstract = {The data and AI technology landscape is in a constant state of rapid change. Businesses know they need to use data to compete, innovate, and succeed, but they struggle to deliver access to more of their data and to do so quickly, easily and cost-effectively. Central to meeting these challenges is a massive shift in foundational data architecture and management---the rise of the data lakehouse. Data lakehouses combine data warehouse functionality with the flexibility and scalability of data lakes, and if they are architected properly, they have the potential to improve data access, agility, and cost efficiency for analytics and AI workloads across industries. They have introduced new, streamlined data processes and have delivered value not previously available with warehouses and data lakes. This survey of 500 full-time IT and data professionals from large enterprises offers fresh insights on data lakehouse adoption and associated cost savings, open table format trends, data mesh implementation for self-service, and use of the data lakehouse in building and improving AI models and applications. The survey also explores AI's impact on jobs and the most pressing issues of the day, reflecting the unique perspectives of this highly technical cohort. The majority of respondents were IT, data, and analytics managers and directors. Data scientists, software engineers, data analysts, and data engineers also contributed to the survey results.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/Q3U89PZ2/whitepaper-2024-state-of-the-data-lakehouse_report.pdf}
}

@misc{SustainableDevelopment,
  title = {{\textbar} {{Sustainable Development}}},
  urldate = {2024-07-19},
  howpublished = {https://sdgs.un.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/IBXUBKQ5/sdgs.un.org.html}
}

@misc{TIOBEIndex,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2024-02-28},
  howpublished = {https://www.tiobe.com/tiobe-index/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/TKAACFMY/tiobe-index.html}
}

@misc{TIOBEIndexa,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2024-07-19},
  howpublished = {https://www.tiobe.com/tiobe-index/programminglanguages\_definition/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/SPFK2QPN/programminglanguages_definition.html}
}

@article{TPC_benchmarks_2000,
  title = {New {{TPC}} Benchmarks for Decision Support and Web Commerce},
  author = {Poess, Meikel and Floyd, Chris},
  year = {2000},
  month = dec,
  journal = {Sigmod Record},
  volume = {29},
  number = {4},
  pages = {64--71},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {0163-5808},
  doi = {10.1145/369275.369291},
  abstract = {For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.},
  issue_date = {Dec. 2000}
}

@misc{TPCCurrentSpecs,
  title = {{{TPC Current Specs}}},
  urldate = {2024-09-12},
  howpublished = {https://www.tpc.org/tpc\_documents\_current\_versions/current\_specifications5.asp},
  file = {/Users/gio-hopsworks/Zotero/storage/MRQI7FCE/current_specifications5.html}
}

@misc{TPCHHomepage,
  title = {{{TPC-H Homepage}}},
  urldate = {2024-09-12},
  howpublished = {https://www.tpc.org/tpch/},
  file = {/Users/gio-hopsworks/Zotero/storage/UUNHFVDY/tpch.html}
}

@misc{transactionprocessingperformancecounciltpcTPCH_v301pdf1993,
  title = {{{TPC-H}}\_v3.0.1.Pdf},
  author = {Transaction Processing Performance Council (TPC)},
  year = {1993/2022},
  publisher = {https://www.tpc.org/TPC\_Documents\_Current\_Versions/pdf/TPC-H\_v3.0.1.pdf},
  urldate = {2024-09-19},
  file = {/Users/gio-hopsworks/Zotero/storage/F3PNL2U3/TPC-H_v3.0.1.pdf}
}

@misc{vinkWroteOneFastest2021,
  type = {Blog},
  title = {I Wrote One of the Fastest {{DataFrame}} Libraries},
  author = {Vink, Ritchie},
  year = {2021},
  month = feb,
  urldate = {2024-02-28},
  howpublished = {https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/},
  file = {/Users/gio-hopsworks/Zotero/storage/FIXIGF98/i-wrote-one-of-the-fastest-dataframe-libraries.html}
}

@misc{WelcomeHomeVim,
  title = {Welcome Home : Vim Online},
  urldate = {2024-09-11},
  howpublished = {https://www.vim.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/T6RH6267/www.vim.org.html}
}

@misc{WhatGreenSoftware2021,
  title = {What Is {{Green Software}}?},
  year = {2021},
  month = oct,
  journal = {Green Software Foundation},
  urldate = {2024-05-28},
  abstract = {Creating a trusted ecosystem of people, standards, tooling, and best practices for building green software.},
  howpublished = {https://greensoftware.foundation/articles/what-is-green-software},
  file = {/Users/gio-hopsworks/Zotero/storage/ADZUW9Y4/what-is-green-software.html}
}

@misc{WhatLakehouse2020,
  title = {What {{Is}} a {{Lakehouse}}?},
  year = {Thu, 01/30/2020 - 09:00},
  journal = {Databricks},
  urldate = {2024-06-14},
  abstract = {Learn more about the new data management paradigm data lakehouses -- its evolution, adoption, common use cases, and its advantages over previous systems and technologies.},
  howpublished = {https://www.databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/DMAGTAIS/what-is-a-data-lakehouse.html}
}

@incollection{white2009hadoopMapReduce,
  title = {Hadoop : The Definitive Guide},
  booktitle = {Hadoop : The Definitive Guide},
  author = {White, Tom (Tom E.)},
  year = {2009},
  edition = {First edition.},
  publisher = {O'Reilly Media, Inc.},
  address = {Sebastopol, California},
  abstract = {Hadoop: The Definitive Guide helps you harness the power of your data. Ideal for processing large datasets, the Apache Hadoop framework is an open source implementation of the MapReduce algorithm on which Google built its empire. This comprehensive resource demonstrates how to use Hadoop to build reliable, scalable, distributed systems: programmers will find details for analyzing large datasets, and administrators will learn how to set up and run Hadoop clusters. Complete with case studies that illustrate how Hadoop solves specific problems, this book helps you: {\textexclamdown}p},
  isbn = {1-306-81746-3},
  langid = {english},
  keywords = {Apache Hadoop}
}

@techreport{Zaharia:EECS-2011-82,
  title = {Resilient {{Distributed Datasets}}: {{A Fault-Tolerant Abstraction}} for {{In-Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2011},
  month = jul,
  number = {UCB/EECS-2011-82},
  institution = {EECS Department, University of California, Berkeley},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KC5XPCLU/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf;/Users/gio-hopsworks/Zotero/storage/ZLWNKHFL/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf}
}

@inproceedings{zaharia2010spark,
  title = {Spark: Cluster Computing with Working Sets},
  booktitle = {Proceedings of the 2nd {{USENIX}} Conference on Hot Topics in Cloud Computing},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2010},
  series = {{{HotCloud}}'10},
  pages = {10},
  publisher = {USENIX Association},
  address = {USA},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}

@article{zahariaApacheSparkUnified2016,
  title = {Apache {{Spark}}: A Unified Engine for Big Data Processing},
  shorttitle = {Apache {{Spark}}},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  year = {2016},
  month = oct,
  journal = {Communications of the ACM},
  volume = {59},
  number = {11},
  pages = {56--65},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2934664},
  urldate = {2024-02-28},
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/HD67UPVM/Zaharia et al. - 2016 - Apache Spark a unified engine for big data proces.pdf}
}
