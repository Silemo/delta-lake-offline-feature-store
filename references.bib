@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2024-03-01},
  howpublished = {https://hadoop.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/Q39ZUKHL/hadoop.apache.org.html}
}

@misc{ApacheHudiVs,
  title = {Apache {{Hudi}} vs {{Delta Lake}} vs {{Apache Iceberg}} - {{Data Lakehouse Feature Comparison}}},
  urldate = {2024-02-28},
  abstract = {A thorough comparison of the Apache Hudi, Delta Lake, and Apache Iceberg data lakehouse projects across features, community, and performance benchmarks. This includes a focus on common use cases such as change data capture (CDC) and data ingestion.},
  howpublished = {https://www.onehouse.ai/blog/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison},
  file = {/Users/gio-hopsworks/Zotero/storage/EWJWIY6K/apache-hudi-vs-delta-lake-vs-apache-iceberg-lakehouse-feature-comparison.html}
}

@misc{ApacheSparkUnified,
  title = {Apache {{Spark}}™ - {{Unified Engine}} for Large-Scale Data Analytics},
  urldate = {2024-03-01},
  howpublished = {https://spark.apache.org/},
  file = {/Users/gio-hopsworks/Zotero/storage/WLV7LSIZ/spark.apache.org.html}
}

@article{armbrustDeltaLakeHighperformance2020,
  title = {Delta Lake: High-Performance {{ACID}} Table Storage over Cloud Object Stores},
  shorttitle = {Delta Lake},
  author = {Armbrust, Michael and Das, Tathagata and Sun, Liwen and Yavuz, Burak and Zhu, Shixiong and Murthy, Mukul and Torres, Joseph and Van Hovell, Herman and Ionescu, Adrian and {\L}uszczak, Alicja and {\'S}witakowski, Micha{\l} and Szafra{\'n}ski, Micha{\l} and Li, Xiao and Ueshin, Takuya and Mokhtar, Mostafa and Boncz, Peter and Ghodsi, Ali and Paranjpye, Sameer and Senster, Pieter and Xin, Reynold and Zaharia, Matei},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {3411--3424},
  issn = {2150-8097},
  doi = {10.14778/3415478.3415560},
  urldate = {2024-02-09},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/ZNKWS6BK/Armbrust et al. - 2020 - Delta lake high-performance ACID table storage ov.pdf}
}

@misc{BenchmarkResultsSpark,
  title = {Benchmark {{Results}} for {{Spark}}, {{Dask}}, {{DuckDB}}, and {{Polars}} --- {{TPC-H Benchmarks}} at {{Scale}}},
  urldate = {2024-02-28},
  howpublished = {https://tpch.coiled.io/},
  file = {/Users/gio-hopsworks/Zotero/storage/UF64X2HH/tpch.coiled.io.html}
}

@article{carboneApacheFlinkStream,
  title = {Apache {{Flink}}™: {{Stream}} and {{Batch Processing}} in a {{Single Engine}}},
  author = {Carbone, Paris and Katsifodimos, Asterios and Ewen, Stephan and Markl, Volker and Haridi, Seif and Tzoumas, Kostas},
  abstract = {Apache Flink1 is an open-source system for processing streaming and batch data. Flink is built on the philosophy that many classes of data processing applications, including real-time analytics, continuous data pipelines, historic data processing (batch), and iterative algorithms (machine learning, graph analysis) can be expressed and executed as pipelined fault-tolerant dataflows. In this paper, we present Flink's architecture and expand on how a (seemingly diverse) set of use cases can be unified under a single execution model.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KI73RD8V/Carbone et al. - Apache Flink™ Stream and Batch Processing in a Si.pdf}
}

@misc{crociDataLakehouseHype2022,
  title = {Data {{Lakehouse}}, beyond the Hype},
  author = {Croci, Daniele},
  year = {2022},
  month = dec,
  journal = {Bitrock},
  urldate = {2024-02-28},
  abstract = {In this blogpost we explore the data lakehouse as new concept that moves data lakes closer to warehouses, to compete in the BI and analytical scenario},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/84YWQSRD/data-lakehouse.html}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  urldate = {2024-02-29},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/I2KVFQJI/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@misc{DremelMadeSimple,
  title = {Dremel Made Simple with {{Parquet}}},
  urldate = {2024-02-29},
  abstract = {Dremel made simple with Parquet},
  howpublished = {https://blog.x.com/engineering/en\_us/a/2013/dremel-made-simple-with-parquet},
  langid = {american}
}

@misc{ebergenUpdatesH2OAi2023,
  title = {Updates to the {{H2O}}.Ai Db-Benchmark!},
  author = {Ebergen, Tom},
  year = {2023},
  month = nov,
  journal = {DuckDB},
  urldate = {2024-02-28},
  abstract = {The H2O.ai db-benchmark has been updated with new results. In addition, the AWS EC2 instance used for benchmarking has been changed to a c6id.metal for improved repeatability and fairness across libraries. DuckDB is the fastest library for both join and group by queries at almost every data size.},
  howpublished = {https://duckdb.org/2023/11/03/db-benchmark-update.html},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/6T3M3CNT/db-benchmark-update.html}
}

@misc{ederUnstructuredData802008,
  title = {Unstructured {{Data}} and the 80 {{Percent Rule}}},
  author = {{EDER}},
  year = {2008},
  month = aug,
  journal = {Breakthrough Analysis},
  urldate = {2024-02-29},
  abstract = {It's a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. But for all of us who cite these figures: Where did they come from? More to the{\dots}},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/3YDK8CIW/unstructured-data-and-the-80-percent-rule.html}
}

@mastersthesis{GebretsadkanKidane1413103,
  title = {Hudi on Hops : {{Incremental}} Processing and Fast Data Ingestion for Hops},
  author = {Gebretsadkan Kidane, Netsanet},
  year = {2019},
  series = {{{TRITA-EECS-EX}}},
  number = {2019:809},
  pages = {49},
  abstract = {In the era of big data, data is flooding from numerous data sources and many companies have been utilizing different types of tools to load and process data from various sources in a data lake. The major challenges where different companies are facing these days are how to update data into an existing dataset without having to read the entire dataset and overwriting it to accommodate the changes which have a negative impact on the performance. Besides this, finding a way to capture and track changed data in a big data lake as the system gets complex with large amounts of data to maintain and query is another challenge. Web platforms such as Hopsworks are also facing these problems without having an efficient mechanism to modify an existing processed results and pull out only changed data which could be useful to meet the processing needs of an organization. The challenge of accommodating row level changes in an efficient and effective manner is solved by integrating Hudi with Hops. This takes advantage of Hudi's upsert mechanism which uses Bloom indexing to significantly speed up the ability of looking up records across partitions. Hudi indexing maps a record key into the file id without scanning over every record in the dataset. In addition, each successful data ingestion is stored in Apache Hudi format stamped with commit timeline. This commit timeline is needed for the incremental processing mainly to pull updated rows since a specified instant of time and obtain change logs from a dataset. Hence, incremental pulls are realized through the monotonically increasing commit time line. Similarly, incremental updates are realized over a time column (key expression) that allows Hudi to update rows based on this time column. HoodieDeltaStreamer utility and DataSource API are used for the integration of Hudi with Hops and Feature store. As a result, this provided a fabulous way of ingesting and extracting row level updates where its performance can further be enhanced by the configurations of the shuffle parallelism and other spark parameter configurations since Hudi is a spark based library.},
  school = {KTH, School of Electrical Engineering and Computer Science (EECS) / KTH, School of Electrical Engineering and Computer Science (EECS)},
  keywords = {Hadoop,Hops,Hudi,Kafka,Spark,SQL,Upsert},
  file = {/Users/gio-hopsworks/Zotero/storage/CCZEDP5V/Gebretsadkan Kidane - 2019 - Hudi on hops  Incremental processing and fast dat.pdf}
}

@misc{HopsworksBatchRealtime2024,
  title = {Hopsworks - {{Batch}} and {{Real-time ML Platform}}},
  year = {2024},
  urldate = {2024-02-09},
  abstract = {Hopsworks is a flexible and modular feature store that provides seamless integration for existing pipelines, superior performance for any SLA, and increased productivity for data and AI teams.},
  howpublished = {https://www.hopsworks.ai/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/UMNPVBHQ/www.hopsworks.ai.html}
}

@inproceedings{lakehouse2021,
  title = {Lakehouse: A New Generation of Open Platforms That Unify Data Warehousing and Advanced Analytics},
  booktitle = {Proceedings of {{CIDR}}},
  author = {Armbrust, Michael and Ghodsi, Ali and Xin, Reynold and Zaharia, Matei},
  year = {2021},
  volume = {8},
  abstract = {Cloud object stores such as Amazon S3 are some of the largest and most cost-effective storage systems on the planet, making them an attractive target to store large data warehouses and data lakes. Unfortunately, their implementation as key-value stores makes it difficult to achieve ACID transactions and high performance: metadata operations such as listing objects are expensive, and consistency guarantees are limited. In this paper, we present Delta Lake, an open source ACID table storage layer over cloud object stores initially developed at Databricks. Delta Lake uses a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets (e.g., the ability to quickly search billions of table partitions for those relevant to a query). It also leverages this design to provide high-level features such as automatic data layout optimization, upserts, caching, and audit logs. Delta Lake tables can be accessed from Apache Spark, Hive, Presto, Redshift and other systems. Delta Lake is deployed at thousands of Databricks customers that process exabytes of data per day, with the largest instances managing exabyte-scale datasets and billions of objects.},
  file = {/Users/gio-hopsworks/Zotero/storage/GCE6RS6H/Armbrust et al. - 2021 - Lakehouse a new generation of open platforms that.pdf}
}

@phdthesis{More862135,
  title = {{{HopsWorks}} : {{A}} Project-Based Access Control Model for {{Hadoop}}},
  author = {Mor{\'e}, Andr{\'e} and Gebremeskel, Ermias},
  year = {2015},
  series = {{{TRITA-ICT-EX}}},
  number = {2015:70},
  abstract = {The growth in the global data gathering capacity is producing a vast amount of data which is getting vaster at an increasingly faster rate. This data properly analyzed can represent great opportunity for businesses, but processing it is a resource-intensive task. Sharing can increase efficiency due to reusability but there are legal and ethical questions that arise when data is shared. The purpose of this thesis is to gain an in depth understanding of the different access control methods that can be used to facilitate sharing, and choose one to implement on a platform that lets user analyze, share, and collaborate on, datasets. The resulting platform uses a project based access control on the API level and a fine-grained role based access control on the file system to give full control over the shared data to the data owner.},
  school = {KTH, School of Information and Communication Technology (ICT) / KTH, School of Information and Communication Technology (ICT) and KTH, School of Information and Communication Technology (ICT)},
  keywords = {Big Data,DataSets,Distributed Computing,Hadoop,Hops,HopsWorks},
  file = {/Users/gio-hopsworks/Zotero/storage/89KVT5ZU/Moré and Gebremeskel - 2015 - HopsWorks  A project-based access control model f.pdf}
}

@techreport{Python_CS-R9526,
  title = {Python Tutorial},
  author = {{van Rossum}, G.},
  year = {1995},
  month = may,
  number = {CS-R9526},
  address = {{Amsterdam}},
  institution = {{Centrum voor Wiskunde en Informatica (CWI)}},
  file = {/Users/gio-hopsworks/Zotero/storage/43WNZ76F/van Rossum - 1995 - Python tutorial.pdf}
}

@book{python-machine-learning,
  title = {Python Machine Learning (3rd Edition)},
  author = {Raschka, Vahid, Sebastian and Mirjalili},
  year = {2019},
  publisher = {{Packt Publishing}},
  isbn = {978-1-78995-575-0}
}

@inproceedings{raasveldtDuckDBEmbeddableAnalytical2019,
  title = {{{DuckDB}}: An {{Embeddable Analytical Database}}},
  shorttitle = {{{DuckDB}}},
  booktitle = {Proceedings of the 2019 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Raasveldt, Mark and M{\"u}hleisen, Hannes},
  year = {2019},
  month = jun,
  pages = {1981--1984},
  publisher = {{ACM}},
  address = {{Amsterdam Netherlands}},
  doi = {10.1145/3299869.3320212},
  urldate = {2024-02-28},
  abstract = {The great popularity of SQLite shows that there is a need for unobtrusive in-process data management solutions. However, there is no such system yet geared towards analytical workloads. We demonstrate DuckDB, a novel data management system designed to execute analytical SQL queries while embedded in another process. In our demonstration, we pit DuckDB against other data management solutions to showcase its performance in the embedded analytics scenario. DuckDB is available as Open Source software under a permissive license.},
  isbn = {978-1-4503-5643-5},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/G72KI4R3/Raasveldt and Mühleisen - 2019 - DuckDB an Embeddable Analytical Database.pdf}
}

@misc{rajaperumalUberEngineeringIncremental2017,
  title = {Uber {{Engineering}}'s {{Incremental Processing Framework}} on {{Hadoop}}},
  author = {Rajaperumal, Prasanna},
  year = {2017},
  month = mar,
  journal = {Uber Blog},
  urldate = {2024-02-29},
  abstract = {Uber Engineering recently built and open sourced Hoodie, an incremental processing framework powering Uber's business critical pipelines on Hadoop.},
  howpublished = {https://www.uber.com/blog/hoodie/},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/29U69TP6/hoodie.html}
}

@techreport{StateDataLakehouse2024,
  title = {State of the {{Data Lakehouse}}},
  year = {2024},
  pages = {17},
  institution = {{Dremio}},
  abstract = {The data and AI technology landscape is in a constant state of rapid change. Businesses know they need to use data to compete, innovate, and succeed, but they struggle to deliver access to more of their data and to do so quickly, easily and cost-effectively. Central to meeting these challenges is a massive shift in foundational data architecture and management---the rise of the data lakehouse. Data lakehouses combine data warehouse functionality with the flexibility and scalability of data lakes, and if they are architected properly, they have the potential to improve data access, agility, and cost efficiency for analytics and AI workloads across industries. They have introduced new, streamlined data processes and have delivered value not previously available with warehouses and data lakes. This survey of 500 full-time IT and data professionals from large enterprises offers fresh insights on data lakehouse adoption and associated cost savings, open table format trends, data mesh implementation for self-service, and use of the data lakehouse in building and improving AI models and applications. The survey also explores AI's impact on jobs and the most pressing issues of the day, reflecting the unique perspectives of this highly technical cohort. The majority of respondents were IT, data, and analytics managers and directors. Data scientists, software engineers, data analysts, and data engineers also contributed to the survey results.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/Q3U89PZ2/whitepaper-2024-state-of-the-data-lakehouse_report.pdf}
}

@misc{TIOBEIndex,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2024-02-28},
  howpublished = {https://www.tiobe.com/tiobe-index/},
  langid = {american},
  file = {/Users/gio-hopsworks/Zotero/storage/TKAACFMY/tiobe-index.html}
}

@misc{vinkWroteOneFastest2021,
  type = {Blog},
  title = {I Wrote One of the Fastest {{DataFrame}} Libraries},
  author = {Vink, Ritchie},
  year = {2021},
  month = feb,
  urldate = {2024-02-28},
  howpublished = {https://www.ritchievink.com/blog/2021/02/28/i-wrote-one-of-the-fastest-dataframe-libraries/},
  file = {/Users/gio-hopsworks/Zotero/storage/FIXIGF98/i-wrote-one-of-the-fastest-dataframe-libraries.html}
}

@techreport{Zaharia:EECS-2011-82,
  title = {Resilient {{Distributed Datasets}}: {{A Fault-Tolerant Abstraction}} for {{In-Memory Cluster Computing}}},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur and Ma, Justin and McCauley, Murphy and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2011},
  month = jul,
  number = {UCB/EECS-2011-82},
  institution = {{EECS Department, University of California, Berkeley}},
  abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/KC5XPCLU/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf;/Users/gio-hopsworks/Zotero/storage/ZLWNKHFL/Zaharia et al. - Resilient Distributed Datasets A Fault-Tolerant A.pdf}
}

@article{zahariaApacheSparkUnified2016,
  title = {Apache {{Spark}}: A Unified Engine for Big Data Processing},
  shorttitle = {Apache {{Spark}}},
  author = {Zaharia, Matei and Xin, Reynold S. and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J. and Ghodsi, Ali and Gonzalez, Joseph and Shenker, Scott and Stoica, Ion},
  year = {2016},
  month = oct,
  journal = {Communications of the ACM},
  volume = {59},
  number = {11},
  pages = {56--65},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/2934664},
  urldate = {2024-02-28},
  abstract = {This open source computing framework unifies streaming, batch, and interactive big data workloads to unlock new applications.},
  langid = {english},
  file = {/Users/gio-hopsworks/Zotero/storage/HD67UPVM/Zaharia et al. - 2016 - Apache Spark a unified engine for big data proces.pdf}
}
